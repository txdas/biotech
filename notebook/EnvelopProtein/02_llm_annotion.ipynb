{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfaa27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4113it [08:08,  8.41it/s]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import tqdm\n",
    "openai_api_key = \"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\"\n",
    "os.environ.update({\"OPENAI_API_KEY\": openai_api_key, \"TOKENIZERS_PARALLELISM\": \"true\"})\n",
    "from langchain.output_parsers.list import ListOutputParser\n",
    "from typing import Dict, Optional, List, Type, TypeVar\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=2000)\n",
    "\n",
    "regexp = re.compile(\"(.*):(.*)\",re.MULTILINE|re.DOTALL)\n",
    "\n",
    "PROMPT_PROTEIN_PAIR_CONTEX = '''\n",
    "There is an paragraph which may contain information about viruses and their associated proteins.\n",
    "Please extract the names of viruses and their related proteins mentioned in the paragraph.\n",
    "If neither viruses nor proteins are mentioned, please return an empty list.\n",
    "Note that don't make up content:\n",
    "{context}\n",
    "\n",
    "The desired format of output is: \n",
    "<virus_name>: <comma_separated_list_of_protein_names_in_the_virus>\n",
    "\n",
    "'''\n",
    "\n",
    "class ProteinsListOutputParser(ListOutputParser):\n",
    "    \"\"\"Parse out comma separated lists.\"\"\"\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return (\n",
    "            \"Your response should be a list of comma separated values, \"\n",
    "            \"eg: `foo, bar, baz`\"\n",
    "        )\n",
    "\n",
    "    def parse(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        # print(text)\n",
    "        lines = text.split(\"\\n\")\n",
    "        lst = []\n",
    "        for line in lines:\n",
    "            match = re.match(regexp, line)\n",
    "            if match:\n",
    "                virus = match.group(1).strip()\n",
    "                proteins = match.group(2).strip()\n",
    "                proteins = re.sub(\"(, ?and|and)\", \",\", proteins).strip(\".\")\n",
    "                proteins = proteins.strip().split(\",\")\n",
    "                if virus and proteins:\n",
    "                    lst.append({\"virus\":virus, \"proteins\":proteins})\n",
    "        return lst\n",
    "\n",
    "ner_output_parser = ProteinsListOutputParser()\n",
    "    \n",
    "def ner(context):\n",
    "    prompt = PromptTemplate(template=PROMPT_PROTEIN_PAIR_CONTEX,\n",
    "                            input_variables=[\"context\"])\n",
    "    _input = prompt.format(context=context)\n",
    "    output = llm.predict(_input)\n",
    "    res = ner_output_parser.parse(output)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "context = \"\"\"Paramyxoviruses including important pathogens like parainfluenza, \n",
    "measles, and Nipah viruses use a receptor binding protein [hemagglutinin-neuraminidase (HN) \n",
    "for parainfluenza] and a fusion protein (F), acting in a complex, to enter cells. \n",
    "\"\"\"\n",
    "empt_ctx = '''\n",
    "We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow \n",
    "from three aspects, i. e., high-resolution visual tokens, high-quality data, and VLM-guided generation.\n",
    "'''\n",
    "# ner(context)\n",
    "tlp = \"/Volumes/PortableSSD/projects/EnvelopProtein/llm_fusion/{}\"\n",
    "papers_info = tlp.format(\"papers.json\")\n",
    "papers_ner = tlp.format(\"papers_ner1.json\")\n",
    "with jsonlines.open(papers_info) as fp:\n",
    "    with jsonlines.open(papers_ner,\"w\") as wp:\n",
    "        for i, v in tqdm.tqdm(enumerate(fp)):\n",
    "            if i<3603:\n",
    "                continue\n",
    "            if v[\"abstract\"]:\n",
    "#                 print(v[\"abstract\"])\n",
    "                for p in ner(v[\"abstract\"]):\n",
    "                    wp.write(p)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae2b6dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measles viruses fusion protein (0, '2450338479')\n",
      "parainfluenza hemagglutinin-neuraminidase (0, '2699875360')\n",
      "Nipah viruses fusion protein (0, '1679387322')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import openai\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "ANSWER = re.compile(\"Answer: (.+)\")\n",
    "URL = re.compile(r\"\\[(https://.*?)\\]\")\n",
    "API_KEY = '73664c0b675e8450f3cf90add93e69820808'\n",
    "MAPPING = {\"gpt-2\": \"gpt-2\",\n",
    "           \"text-davinci-003\": \"p50k_base\",\n",
    "           \"gpt-3.5-turbo\": \"p50k_base\",\n",
    "           \"gpt-4\": \"cl100k_base\",\n",
    "           \"default\": \"p50k_base\"}\n",
    "genegpt = '''\n",
    "Hello. Your task is to use NCBI APIs to answer genomic questions. You can call the APIs by writing URLs like\n",
    "\"[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/{esearch/efetch}.fcgi?db={gene/protein}&retmax=10&api_key={API_KEY}&{term/id}=\n",
    "{term/id}]\". Here are some examples:\n",
    "Question: What is the official gene ID of Vesicular stomatitis virus G protein?\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=protein&retmax=3&retmode=json&api_key={API_KEY}&term=Vesicular stomatitis virus G protein]->[{API_CALL}]\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=protein&rettype=gb&retmode=text&api_key={API_KEY}&id={ids}]->[{API_CALL}]\n",
    "Answer: 61840\n",
    "Question: What is the official gene ID of Influenza A virus matix protein?\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=protein&retmax=3&retmode=json&api_key={API_KEY}&term=Vesicular stomatitis virus G protein]->[{API_CALL}]\n",
    "Answer: 89573776\n",
    "Question: %s\n",
    "'''\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "def extract(s):\n",
    "    if re.match(ANSWER, s):\n",
    "        hit = re.match(ANSWER, s)\n",
    "        return 2, hit.group(1)\n",
    "    elif re.match(URL, s):\n",
    "        hit = re.match(URL, s)\n",
    "        return 1, hit.group(1)\n",
    "    return 0, \"\"\n",
    "\n",
    "\n",
    "def fetch(url, context=None):\n",
    "    if \"ids\" in context:\n",
    "        content = requests.get(url.format(API_KEY=API_KEY, ids=context[\"ids\"]))\n",
    "    else:\n",
    "        content = requests.get(url.format(API_KEY=API_KEY))\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def token_length(message, model=\"gpt-3.5-turbo\"):\n",
    "    name = MAPPING.get(model) if model in MAPPING else MAPPING.get(\"default\")\n",
    "    encoding = tiktoken.get_encoding(name)\n",
    "    token_integers = encoding.encode(message)\n",
    "    num_tokens = len(token_integers)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def completion(context, engine=\"gpt-3.5-turbo\", **kwargs):  #\n",
    "    try:\n",
    "        max_tokens = 1024\n",
    "        size = token_length(context, engine)\n",
    "        if size + max_tokens > 4096:\n",
    "            return \"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{context}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=kwargs.get(\"stop\", [\"\\n\\n\"])\n",
    "        )\n",
    "#         print(response)\n",
    "        return response.choices[0]\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def main(question):\n",
    "    finished = False\n",
    "    prompt = genegpt % question\n",
    "    ret, context = (-1, \"init status.\"), {}\n",
    "    while not finished:\n",
    "        res = completion(prompt, stop=[\"\\n\\n\", \"->\"])  # model=\"code-davinci-002\",\n",
    "        # res = bio.api.lm.chat(prompt, stop=[\"\\n\\n\", \"->\"])\n",
    "        if res and res.finish_reason == \"stop\":\n",
    "            text = res.message.content\n",
    "            lastp = text.split(\"\\n\")[-1]\n",
    "            code, content = extract(lastp)\n",
    "            if code == 1:\n",
    "                url = content.format(API_KEY=API_KEY)\n",
    "#                 print(\"fetch the url[%s]\\n\\t context[%s]\" % (url, context))\n",
    "                api_result = fetch(url, context)\n",
    "                if api_result:\n",
    "                    raw = api_result.encode(\"utf-8\")\n",
    "#                     print(raw)\n",
    "                    text = f\"{text}>[{raw}]\\n\"\n",
    "                    try:\n",
    "                        obj = json.loads(api_result)\n",
    "                        if \"esearchresult\" in obj and \"idlist\" in obj[\"esearchresult\"]:\n",
    "                            context[\"ids\"] = \",\".join(obj[\"esearchresult\"][\"idlist\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                prompt += text\n",
    "            elif code == 2:\n",
    "                finished = True\n",
    "                ret = 0, content\n",
    "            else:\n",
    "                finished = True\n",
    "                ret = 1, \"Parse gpt result error!\"\n",
    "        else:\n",
    "            finished = True\n",
    "            ret = 1, \"Call the gpt api error!\"\n",
    "    return ret\n",
    "\n",
    "lst = [(\"measles viruses\", \"fusion protein\"), \n",
    "       (\"parainfluenza\", \"hemagglutinin-neuraminidase\"),\n",
    "       (\"Nipah viruses\",\"fusion protein\") ]\n",
    "for virus, protein in lst:\n",
    "    question = f\"What is the GI number of {virus} {protein}?\"\n",
    "    print(virus, protein, main(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0792729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMPV (0, 'Human metapneumovirus') (0, '162145')\n",
      "MLV (0, 'Murine leukemia virus') (0, '11786')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import openai\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "ANSWER = re.compile(\"Answer: (.+)\")\n",
    "URL = re.compile(r\"\\[(https://.*?)\\]\")\n",
    "genegpt = '''\n",
    "Hello. Your task is to use NCBI APIs to answer genomic questions. You can call the APIs by writing URLs like\n",
    "\"[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/{esearch/efetch}.fcgi?db={taxonomy}&retmax=10&api_key={API_KEY}&{term/id}=\n",
    "{term/id}]\". Here are some examples:\n",
    "Question: What is the fullname of RSV?\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=taxonomy&retmax=3&retmode=json&api_key={API_KEY}&term=RSV]->[{API_CALL}]\n",
    "Answer: Respiratory syncytial virus\n",
    "Question: What is the Taxonomy ID of HRSV?\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=taxonomy&retmax=3&retmode=json&api_key={API_KEY}&term=HRSV]->[{API_CALL}]\n",
    "[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=taxonomy&rettype=gb&retmode=text&api_key={API_KEY}&id={ids}]->[{API_CALL}]\n",
    "Answer: 11250\n",
    "Question: %s'''\n",
    "API_KEY = '73664c0b675e8450f3cf90add93e69820808'\n",
    "MAPPING = {\"gpt-2\": \"gpt-2\",\n",
    "           \"text-davinci-003\": \"p50k_base\",\n",
    "           \"gpt-3.5-turbo\": \"p50k_base\",\n",
    "           \"gpt-4\": \"cl100k_base\",\n",
    "           \"default\": \"p50k_base\"}\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract(s):\n",
    "    if re.match(ANSWER, s):\n",
    "        hit = re.match(ANSWER, s)\n",
    "        return 2, hit.group(1)\n",
    "    elif re.match(URL, s):\n",
    "        hit = re.match(URL, s)\n",
    "        return 1, hit.group(1)\n",
    "    return 0, \"\"\n",
    "\n",
    "def fetch(url, context=None):\n",
    "    if \"ids\" in context:\n",
    "        content = requests.get(url.format(API_KEY=API_KEY, ids=context[\"ids\"]))\n",
    "    else:\n",
    "        content = requests.get(url.format(API_KEY=API_KEY))\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def token_length(message, model=\"gpt-3.5-turbo\"):\n",
    "    name = MAPPING.get(model) if model in MAPPING else MAPPING.get(\"default\")\n",
    "    encoding = tiktoken.get_encoding(name)\n",
    "    token_integers = encoding.encode(message)\n",
    "    num_tokens = len(token_integers)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def completion(context, engine=\"gpt-3.5-turbo\", **kwargs):  #\n",
    "    try:\n",
    "        max_tokens = 1024\n",
    "        size = token_length(context, engine)\n",
    "        if size + max_tokens > 4096:\n",
    "            return \"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{context}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=kwargs.get(\"stop\", [\"\\n\\n\"])\n",
    "        )\n",
    "#         print(response)\n",
    "        return response.choices[0]\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "def main(question):\n",
    "    finished = False\n",
    "    prompt = genegpt % question\n",
    "    ret, context = (-1, \"init status.\"), {}\n",
    "    while not finished:\n",
    "        res = completion(prompt, stop=[\"\\n\\n\", \"->\"])  # model=\"code-davinci-002\",\n",
    "        # res = bio.api.lm.chat(prompt, stop=[\"\\n\\n\", \"->\"])\n",
    "        if res and res.finish_reason == \"stop\":\n",
    "            text = res.message.content\n",
    "            lastp = text.split(\"\\n\")[-1]\n",
    "            code, content = extract(lastp)\n",
    "            if code == 1:\n",
    "                url = content.format(API_KEY=API_KEY)\n",
    "#                 print(\"fetch the url[%s]\\n\\t context[%s]\" % (url, context))\n",
    "                api_result = fetch(url, context)\n",
    "                if api_result:\n",
    "                    raw = api_result.encode(\"utf-8\")\n",
    "#                     print(raw)\n",
    "                    text = f\"{text}>[{raw}]\\n\"\n",
    "                    try:\n",
    "                        obj = json.loads(api_result)\n",
    "                        if \"esearchresult\" in obj and \"idlist\" in obj[\"esearchresult\"]:\n",
    "                            context[\"ids\"] = \",\".join(obj[\"esearchresult\"][\"idlist\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                prompt += text\n",
    "            elif code == 2:\n",
    "                finished = True\n",
    "                ret = 0, content\n",
    "            else:\n",
    "                finished = True\n",
    "                ret = 1, \"Parse gpt result error!\"\n",
    "        else:\n",
    "            finished = True\n",
    "            ret = 1, \"Call the gpt api error!\"\n",
    "    return ret\n",
    "\n",
    "lst = [(\"HMPV\",\"\"), # 162145\n",
    "       (\"MLV\",\"fusion protein\") ] # 11786\n",
    "for virus, protein in lst:\n",
    "    question_name = f\"What is the fullname of {virus}?\"\n",
    "    question_id = f\"What is the Taxonomy ID of {virus}?\"\n",
    "    print(virus, main(question_name), main(question_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bff81806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████▊                                              | 859/1471 [1:41:32<1:12:20,  7.09s/it]\n",
      " 42%|███████████████████████████████████████████████                                                                  | 612/1471 [53:56<1:00:10,  4.20s/it]"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import re\n",
    "tlp = \"/Volumes/PortableSSD/projects/EnvelopProtein/llm_fusion/{}\"\n",
    "regexp = re.compile(\"(.*)(\\([A-Z 0-9]+\\))(.*)\")\n",
    "regexp_abbr = re.compile(\"[A-Za-z0-9\\-]+$\")\n",
    "regexp_taxid = re.compile(\"(^| )([0-9]+)\")\n",
    "\n",
    "def parse_taxid(v):\n",
    "    match = regexp_taxid.search(v)\n",
    "    if match:\n",
    "        taxid = match.group(2)\n",
    "        return  None if len(taxid)<3 else taxid\n",
    "\n",
    "def parse_name(raw):\n",
    "    raw = raw.strip()\n",
    "    match = regexp_abbr.match(raw)\n",
    "    if match:\n",
    "        abbr = match.group(0)\n",
    "        return abbr, \"\"\n",
    "    match = regexp.match(raw)\n",
    "    if match:\n",
    "        abbr= match.group(2)[1:-1]\n",
    "        prefix, suffix = match.group(1).strip(), match.group(3).strip()\n",
    "        full = prefix if prefix else sufffix\n",
    "        return abbr, full\n",
    "    elif raw.endswith(\"virus\"):\n",
    "        return \"\", raw\n",
    "    else:\n",
    "        return raw, \"\"\n",
    "d = {}\n",
    "with jsonlines.open(tlp.format(\"papers_ner.json\")) as fp:\n",
    "    for v in fp:\n",
    "        k = v[\"virus\"].strip()\n",
    "        if k in d:\n",
    "            d[k].update(v[\"proteins\"])\n",
    "        else:\n",
    "            cnt = Counter(v[\"proteins\"])\n",
    "            d[k]=cnt\n",
    "lst = sorted(d.items(), key=lambda x:-sum(x[1].values()))\n",
    "bar = tqdm.tqdm(total=len(lst))\n",
    "with jsonlines.open(tlp.format(\"papers_ner_normalize1.json\"), \"w\") as wp:\n",
    "    for i,(k, v) in enumerate(lst):\n",
    "        if i<859:\n",
    "            continue\n",
    "        abbr, full = parse_name(k)\n",
    "        if not full:\n",
    "            question_name = f\"What is the fullname of {abbr}?\"\n",
    "            question_id = f\"What is the Taxonomy ID of {abbr}?\"\n",
    "            full =  main(question_name)\n",
    "            full =  full[1] if full[0] ==0 else \"\"\n",
    "            taxid = main(question_id)\n",
    "            taxid = taxid[1] if taxid[0] ==0 else \"\"\n",
    "        else:\n",
    "            question_id = f\"What is the Taxonomy ID of {full}?\"\n",
    "            taxid = main(question_id)\n",
    "            taxid = taxid[1] if taxid[0] ==0 else \"\"\n",
    "        bar.update()\n",
    "        v = {\"raw\":k, \"abbr\":abbr, \"full\":full, \"taxid\":taxid, \"proteins\":v}\n",
    "        wp.write(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a17c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(tlp.format(\"papers_ner_normalize_final.json\")) as fp:\n",
    "    with jsonlines.open(tlp.format(\"papers_ner_normalize.json\"), \"w\") as wp:\n",
    "        for v in fp:\n",
    "            v[\"taxid\"] = parse_taxid(v[\"taxid\"])\n",
    "            wp.write(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e26bb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 {'eastern equine encephalitis virus', 'human sars coronavirus', 'borna disease virus 2', 'walleye dermal sarcoma virus', 'snakehead retrovirus', 'vesicular stomatitis arizona virus', 'yellow fever virus', 'quaranfil quaranjavirus', 'bovine foamy virus', 'dengue virus 1', 'onyong-nyong virus', 'human spumaretrovirus', 'feline foamy virus', 'blue river virus', 'cocal virus', 'dengue virus 3', 'human t-lymphotropic virus 1', 'borna disease virus 1', 'punta toro phlebovirus', 'european bat lyssavirus', 'australian bat lyssavirus', 'isfahan virus', 'dugbe virus', 'louping ill virus', 'murray valley encephalitis virus', 'pichinde virus', 'mers coronavirus', 'chapare hemorrhagic fever virus', 'simian retrovirus', 'tick-borne powassan virus', 'hiv1', 'hiv2', 'dengue virus 4', 'whitewater arroyo virus', 'sars2 coronavirus', 'kunjin virus', 'baboon endogenous retrovirus', 'maedi-visna virus', 'oropouche virus', 'lake victoria marburgvirus', 'variegated squirrel bornavirus', 'human t-lymphotropic virus 2', 'avian leukosis virus'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "s1 = set()\n",
    "s2 = set()\n",
    "with open(\"/Volumes/PortableSSD/projects/EnvelopProtein/nc_zf/env.csv\") as fp:\n",
    "    reader = csv.DictReader(fp)\n",
    "    for v in reader:\n",
    "        s1.add(v[\"Name\"].lower().strip())\n",
    "with jsonlines.open(tlp.format(\"papers_ner_normalize.json\")) as fp:\n",
    "    for v in fp:\n",
    "        s2.add(v[\"abbr\"].lower().strip())\n",
    "        s2.add(v[\"full\"].lower().strip())\n",
    "print(len(s1-s2),s1-s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "788b8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.42s/it]\u001b[A\n",
      "2it [00:03,  1.74s/it]\u001b[A\n",
      "3it [00:04,  1.51s/it]\u001b[A\n",
      "4it [00:06,  1.55s/it]\u001b[A\n",
      "5it [00:07,  1.40s/it]\u001b[A\n",
      "6it [00:08,  1.32s/it]\u001b[A\n",
      "7it [00:10,  1.42s/it]\u001b[A\n",
      "8it [00:11,  1.34s/it]\u001b[A\n",
      "9it [00:12,  1.31s/it]\u001b[A\n",
      "10it [00:13,  1.27s/it]\u001b[A\n",
      "11it [00:14,  1.23s/it]\u001b[A\n",
      "12it [00:16,  1.20s/it]\u001b[A\n",
      "13it [00:17,  1.18s/it]\u001b[A\n",
      "14it [00:18,  1.27s/it]\u001b[A\n",
      "15it [00:19,  1.23s/it]\u001b[A\n",
      "16it [00:21,  1.24s/it]\u001b[A\n",
      "17it [00:22,  1.34s/it]\u001b[A\n",
      "18it [00:24,  1.42s/it]\u001b[A\n",
      "19it [00:25,  1.39s/it]\u001b[A\n",
      "20it [00:26,  1.35s/it]\u001b[A\n",
      "21it [00:28,  1.33s/it]\u001b[A\n",
      "22it [00:29,  1.26s/it]\u001b[A\n",
      "23it [00:30,  1.32s/it]\u001b[A\n",
      "24it [00:31,  1.30s/it]\u001b[A\n",
      "25it [00:33,  1.29s/it]\u001b[A\n",
      "26it [00:34,  1.28s/it]\u001b[A\n",
      "27it [00:35,  1.26s/it]\u001b[A\n",
      "28it [00:36,  1.28s/it]\u001b[A\n",
      "29it [00:38,  1.38s/it]\u001b[A\n",
      "30it [00:39,  1.31s/it]\u001b[A\n",
      "31it [00:41,  1.34s/it]\u001b[A\n",
      "32it [00:43,  1.57s/it]\u001b[A\n",
      "33it [00:44,  1.52s/it]\u001b[A\n",
      "34it [00:46,  1.55s/it]\u001b[A\n",
      "35it [00:47,  1.44s/it]\u001b[A\n",
      "36it [00:49,  1.51s/it]\u001b[A\n",
      "37it [00:50,  1.42s/it]\u001b[A\n",
      "38it [00:51,  1.39s/it]\u001b[A\n",
      "39it [00:53,  1.46s/it]\u001b[A\n",
      "40it [00:54,  1.50s/it]\u001b[A\n",
      "41it [00:56,  1.51s/it]\u001b[A\n",
      "42it [00:57,  1.47s/it]\u001b[A\n",
      "43it [00:59,  1.52s/it]\u001b[A\n",
      "44it [01:00,  1.53s/it]\u001b[A\n",
      "45it [01:02,  1.46s/it]\u001b[A\n",
      "46it [01:03,  1.40s/it]\u001b[A\n",
      "47it [01:05,  1.45s/it]\u001b[A\n",
      "48it [01:06,  1.41s/it]\u001b[A\n",
      "49it [01:07,  1.37s/it]\u001b[A\n",
      "50it [01:09,  1.44s/it]\u001b[A\n",
      "51it [01:10,  1.49s/it]\u001b[A\n",
      "52it [01:12,  1.38s/it]\u001b[A\n",
      "53it [01:14,  1.59s/it]\u001b[A\n",
      "54it [01:15,  1.60s/it]\u001b[A\n",
      "55it [01:16,  1.50s/it]\u001b[A\n",
      "56it [01:18,  1.52s/it]\u001b[A\n",
      "57it [01:20,  1.55s/it]\u001b[A\n",
      "58it [01:21,  1.57s/it]\u001b[A\n",
      "59it [01:23,  1.64s/it]\u001b[A\n",
      "60it [01:24,  1.50s/it]\u001b[A\n",
      "61it [01:26,  1.49s/it]\u001b[A\n",
      "62it [01:27,  1.43s/it]\u001b[A\n",
      "63it [01:29,  1.48s/it]\u001b[A\n",
      "64it [01:30,  1.42s/it]\u001b[A\n",
      "65it [01:32,  1.48s/it]\u001b[A\n",
      "66it [01:33,  1.61s/it]\u001b[A\n",
      "67it [01:35,  1.51s/it]\u001b[A\n",
      "68it [01:36,  1.44s/it]\u001b[A\n",
      "69it [01:37,  1.39s/it]\u001b[A\n",
      "70it [01:39,  1.41s/it]\u001b[A\n",
      "71it [01:40,  1.32s/it]\u001b[A\n",
      "72it [01:41,  1.38s/it]\u001b[A\n",
      "73it [01:43,  1.49s/it]\u001b[A\n",
      "74it [01:44,  1.39s/it]\u001b[A\n",
      "75it [01:46,  1.49s/it]\u001b[A\n",
      "76it [01:48,  1.52s/it]\u001b[A\n",
      "77it [01:49,  1.45s/it]\u001b[A\n",
      "78it [01:50,  1.50s/it]\u001b[A\n",
      "79it [01:52,  1.53s/it]\u001b[A\n",
      "80it [01:54,  1.55s/it]\u001b[A\n",
      "81it [01:55,  1.44s/it]\u001b[A\n",
      "82it [01:56,  1.42s/it]\u001b[A\n",
      "83it [01:57,  1.38s/it]\u001b[A\n",
      "84it [01:59,  1.44s/it]\u001b[A\n",
      "85it [02:00,  1.39s/it]\u001b[A\n",
      "86it [02:03,  1.65s/it]\u001b[A\n",
      "87it [02:04,  1.54s/it]\u001b[A\n",
      "88it [02:05,  1.41s/it]\u001b[A\n",
      "89it [02:06,  1.44s/it]\u001b[A\n",
      "90it [02:08,  1.52s/it]\u001b[A\n",
      "91it [02:10,  1.46s/it]\u001b[A\n",
      "92it [02:11,  1.41s/it]\u001b[A\n",
      "93it [02:12,  1.43s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tqdm \n",
    "def get_taxid(term):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=taxonomy&retmax=3&retmode=json&api_key={API_KEY}&term={term}\"\n",
    "    content = requests.get(url)\n",
    "    body = json.loads(content.text)\n",
    "    if \"esearchresult\" in body and \"idlist\" in body[\"esearchresult\"]:\n",
    "        return body[\"esearchresult\"][\"idlist\"][0] if body[\"esearchresult\"][\"idlist\"] else \"\"\n",
    "with open(\"/Volumes/PortableSSD/projects/EnvelopProtein/nc_zf/env.csv\") as fp:\n",
    "    with jsonlines.open(\"/Volumes/PortableSSD/projects/EnvelopProtein/nc_zf/env_taxid.json\", \"w\") as wp:\n",
    "        reader = csv.DictReader(fp)\n",
    "        for v in tqdm.tqdm(reader):\n",
    "            v[\"taxid\"]= get_taxid(v[\"Name\"])\n",
    "            wp.write(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb111191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40270'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_taxid(\"Snakehead Retrovirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09b097e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>abbr</th>\n",
       "      <th>full</th>\n",
       "      <th>taxid</th>\n",
       "      <th>proteins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RSV</td>\n",
       "      <td>RSV</td>\n",
       "      <td>Respiratory syncytial virus</td>\n",
       "      <td>12814.0</td>\n",
       "      <td>{'F glycoprotein': 18, '': 10, 'F': 65, 'G': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sendai virus</td>\n",
       "      <td></td>\n",
       "      <td>Sendai virus</td>\n",
       "      <td>11234.0</td>\n",
       "      <td>{'F glycoprotein': 5, 'M': 3, ' HN': 22, ' F':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newcastle disease virus (NDV)</td>\n",
       "      <td>NDV</td>\n",
       "      <td>Newcastle disease virus</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>{'HN': 6, ' F': 11, ' VSV-G': 1, 'FRP-1/CD98':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newcastle disease virus</td>\n",
       "      <td></td>\n",
       "      <td>Newcastle disease virus</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>{'hemagglutinin-neuraminidase': 7, ' fusion': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>measles virus</td>\n",
       "      <td></td>\n",
       "      <td>measles virus</td>\n",
       "      <td>11234.0</td>\n",
       "      <td>{'hemagglutinin': 17, ' fusion': 19, 'F protei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>pilot whale morbillivirus</td>\n",
       "      <td></td>\n",
       "      <td>pilot whale morbillivirus</td>\n",
       "      <td>119020.0</td>\n",
       "      <td>{'phosphoprotein': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Anguillid herpesvirus 1</td>\n",
       "      <td>Anguillid herpesvirus 1</td>\n",
       "      <td>Anguillid herpesvirus 1</td>\n",
       "      <td>150286.0</td>\n",
       "      <td>{'hemagglutinin-esterase (HE) protein': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>baboon reovirus (BRV)</td>\n",
       "      <td>BRV</td>\n",
       "      <td>baboon reovirus</td>\n",
       "      <td>75888.0</td>\n",
       "      <td>{'p15': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>thogotovirus</td>\n",
       "      <td>thogotovirus</td>\n",
       "      <td>Thogotovirus</td>\n",
       "      <td>35323.0</td>\n",
       "      <td>{'GP75': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>Duck hepatitis B virus (DHBV)</td>\n",
       "      <td>DHBV</td>\n",
       "      <td>Duck hepatitis B virus</td>\n",
       "      <td>12639.0</td>\n",
       "      <td>{'viral reverse transcriptase': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1471 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                raw                     abbr  \\\n",
       "0                               RSV                      RSV   \n",
       "1                      Sendai virus                            \n",
       "2     Newcastle disease virus (NDV)                      NDV   \n",
       "3           Newcastle disease virus                            \n",
       "4                     measles virus                            \n",
       "...                             ...                      ...   \n",
       "1466      pilot whale morbillivirus                            \n",
       "1467        Anguillid herpesvirus 1  Anguillid herpesvirus 1   \n",
       "1468          baboon reovirus (BRV)                      BRV   \n",
       "1469                   thogotovirus             thogotovirus   \n",
       "1470  Duck hepatitis B virus (DHBV)                     DHBV   \n",
       "\n",
       "                             full     taxid  \\\n",
       "0     Respiratory syncytial virus   12814.0   \n",
       "1                    Sendai virus   11234.0   \n",
       "2         Newcastle disease virus   11250.0   \n",
       "3         Newcastle disease virus   11250.0   \n",
       "4                   measles virus   11234.0   \n",
       "...                           ...       ...   \n",
       "1466    pilot whale morbillivirus  119020.0   \n",
       "1467      Anguillid herpesvirus 1  150286.0   \n",
       "1468              baboon reovirus   75888.0   \n",
       "1469                 Thogotovirus   35323.0   \n",
       "1470       Duck hepatitis B virus   12639.0   \n",
       "\n",
       "                                               proteins  \n",
       "0     {'F glycoprotein': 18, '': 10, 'F': 65, 'G': 1...  \n",
       "1     {'F glycoprotein': 5, 'M': 3, ' HN': 22, ' F':...  \n",
       "2     {'HN': 6, ' F': 11, ' VSV-G': 1, 'FRP-1/CD98':...  \n",
       "3     {'hemagglutinin-neuraminidase': 7, ' fusion': ...  \n",
       "4     {'hemagglutinin': 17, ' fusion': 19, 'F protei...  \n",
       "...                                                 ...  \n",
       "1466                              {'phosphoprotein': 1}  \n",
       "1467         {'hemagglutinin-esterase (HE) protein': 1}  \n",
       "1468                                         {'p15': 1}  \n",
       "1469                                        {'GP75': 1}  \n",
       "1470                 {'viral reverse transcriptase': 1}  \n",
       "\n",
       "[1471 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(tlp.format(\"papers_ner_normalize.json\"),lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a0260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

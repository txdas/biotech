{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efb688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7554],\n",
      "        [-2.1592],\n",
      "        [-1.3452],\n",
      "        [-1.3751],\n",
      "        [-1.3177],\n",
      "        [-1.2945],\n",
      "        [-1.7633],\n",
      "        [-1.8098],\n",
      "        [-1.7569],\n",
      "        [-0.3740]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▎                                                                                                     | 1005/10000 [00:30<04:50, 30.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:1000 Loss:1.6879672874212266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▋                                                                                          | 2008/10000 [00:58<03:10, 42.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:2000 Loss:0.3106649994617328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████████▉                                                                               | 3007/10000 [01:28<02:54, 40.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:3000 Loss:0.0429533090214245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████▏                                                                   | 4004/10000 [01:55<04:29, 22.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:4000 Loss:0.036083709297236054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████▌                                                        | 5004/10000 [02:26<02:38, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:5000 Loss:0.03249288568343036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████▊                                             | 6006/10000 [02:52<01:38, 40.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:6000 Loss:0.03009747034916654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████▏                                 | 7007/10000 [03:23<01:29, 33.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:7000 Loss:0.027860181177733465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████▍                      | 8003/10000 [03:57<01:01, 32.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:8000 Loss:0.025588324351701885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████▎              | 8698/10000 [04:19<00:31, 41.16it/s]"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn, optim,utils\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self,target=0):\n",
    "        super(MSELoss, self).__init__()\n",
    "        self.target = target\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算均方误差\n",
    "        return torch.mean((x-self.target)**2)\n",
    "\n",
    "class OneHotEncoder(object):\n",
    "    \n",
    "    def __init__(self,seq_len):\n",
    "        self.seq_len=seq_len\n",
    "        self.nuc_d = {'a':[1,0,0,0],\n",
    "             'c':[0,1,0,0],\n",
    "             'g':[0,0,1,0],\n",
    "             't':[0,0,0,1],\n",
    "             'n':[0,0,0,0]}\n",
    "    \n",
    "    def __call__(self,seq):\n",
    "        seq = seq[:self.seq_len].lower()\n",
    "        return np.array([self.nuc_d[x] for x in seq])\n",
    "\n",
    "class SampleNoGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        dist = torch.distributions.Categorical(x[...,0])\n",
    "        pwm_sample = torch.nn.functional.one_hot(dist.sample())\n",
    "        return pwm_sample\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g \n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len=44):\n",
    "        super(CNN, self).__init__()\n",
    "        self.seq_len=seq_len\n",
    "        self.model=self.cnn_model()\n",
    "    \n",
    "    def cnn_model(self,):\n",
    "        model = nn.Sequential()\n",
    "        model.append(nn.Conv1d(in_channels=4, out_channels=100, kernel_size=(5,), padding=2))\n",
    "        model.append(nn.ReLU())\n",
    "        model.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(5,), padding=2))\n",
    "        model.append(nn.ReLU())\n",
    "        model.append(nn.Dropout(p=0.3))\n",
    "        model.append(nn.Conv1d(in_channels=100, out_channels=200, kernel_size=(5,), padding=2))\n",
    "        model.append(nn.ReLU())\n",
    "        model.append(nn.BatchNorm1d(200))\n",
    "        model.append(nn.Dropout(p=0.3))\n",
    "        model.append(nn.Flatten())\n",
    "        model.append(nn.Linear(200*seq_len,100))\n",
    "        model.append(nn.ReLU())\n",
    "        model.append(nn.Dropout(p=0.3))\n",
    "        model.add_module(\"linear\",nn.Linear(100,1))\n",
    "        return model\n",
    "    def forward(self, x):\n",
    "        x= torch.permute(x,(0,2,1))\n",
    "        return self.model(x)\n",
    "    \n",
    "class MyInit(object):\n",
    "    def __init__(self, templates,seq_length=26,p_init=0.5):\n",
    "        self.templates = templates\n",
    "        self.seq_length = seq_length\n",
    "        self.p_init = p_init\n",
    "\n",
    "    def __call__(self, module):\n",
    "        encoder = OneHotEncoder(self.seq_length)\n",
    "        if hasattr(module, 'template'):\n",
    "            onehot_templates = np.concatenate([encoder(template).reshape((1, self.seq_length, 4, 1)) \n",
    "                                               for template in self.templates], axis=0)\n",
    "            for i in range(len(self.templates)) :\n",
    "                template = self.templates[i]\n",
    "                for j in range(len(template)) :\n",
    "                    if template[j] != 'N' :\n",
    "                        if template[j] != 'X' :\n",
    "                            nt_ix = np.argmax(onehot_templates[i, j, :, 0])\n",
    "                            onehot_templates[i, j, :, :] = -4\n",
    "                            onehot_templates[i, j, nt_ix, :] = 10\n",
    "                        else :\n",
    "                            onehot_templates[i, j, :, :] = -1\n",
    "            module.template.data = torch.tensor(onehot_templates) \n",
    "        if hasattr(module, 'mask'):\n",
    "            onehot_masks = np.zeros((len(self.templates), self.seq_length, 4, 1))\n",
    "            for i in range(len(self.templates)) :\n",
    "                template = self.templates[i]\n",
    "                for j in range(len(template)) :\n",
    "                    if template[j] == 'N' :\n",
    "                        onehot_masks[i, j, :, :] = 1.0\n",
    "            module.mask.data = torch.tensor(onehot_masks)\n",
    "        if hasattr(module, 'pwm'):\n",
    "            on_logit = np.log(self.p_init / (1. - self.p_init))\n",
    "            p_off = (1. - self.p_init) / 3.\n",
    "            off_logit = np.log(p_off / (1. - p_off))\n",
    "            nn.init.xavier_uniform_(module.pwm.data)\n",
    "        \n",
    "\n",
    "class DNAPWM(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_sequences=10,seq_length=26):\n",
    "        super(DNAPWM, self).__init__()\n",
    "        self.n_sequences=n_sequences\n",
    "        self.seq_length=seq_length\n",
    "        self.template = nn.Parameter(torch.zeros((n_sequences, seq_length, 4, 1)),requires_grad=False)\n",
    "        self.mask = nn.Parameter(torch.zeros((n_sequences, seq_length, 4, 1)),requires_grad=False)\n",
    "        self.pwm = nn.Parameter(torch.randn((n_sequences, seq_length, 4, 1)),requires_grad=True)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=100, kernel_size=(5,), padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(5,), padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv1d(in_channels=100, out_channels=200, kernel_size=(5,), padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(200*118,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3))\n",
    "        self.predictor.add_module(\"linear\",nn.Linear(100,1))\n",
    "        self.init()\n",
    "    \n",
    "    def init(self,fpath=\"/Users/john/data/models/sev_pl3_5.pt\"):\n",
    "        save_model = torch.load(fpath)\n",
    "        modules = {}\n",
    "        for name, module in save_model.model.named_modules():\n",
    "            modules[name] = module\n",
    "        for name,module in self.predictor.named_modules():\n",
    "            if isinstance(module,(nn.Conv1d,nn.Linear,nn.BatchNorm1d)):\n",
    "                module.weight.data.copy_(modules[name].weight.data)\n",
    "                module.weight.requires_grad=False\n",
    "                module.bias.data.copy_(modules[name].bias.data)\n",
    "                module.bias.requires_grad=False\n",
    "            if isinstance(module,nn.BatchNorm1d):\n",
    "                module.running_mean.data.copy_(modules[name].running_mean.data)\n",
    "                module.running_mean.requires_grad=False\n",
    "                module.running_var.data.copy_(modules[name].running_var.data)\n",
    "                module.running_var.requires_grad=False\n",
    "    \n",
    "    def forward(self,):\n",
    "        pwm_logits = self.pwm * self.mask + self.template\n",
    "        pwm = nn.functional.softmax(pwm_logits, dim=2)\n",
    "        if self.training:\n",
    "#             dist = torch.distributions.Categorical(pwm[...,0])\n",
    "#             pwm_sample = torch.nn.functional.one_hot(dist.sample())\n",
    "            pwm_sample=SampleNoGradient.apply(pwm)\n",
    "        else:\n",
    "            sample = torch.argmax(pwm[...,0],dim=2)\n",
    "            pwm_sample = torch.nn.functional.one_hot(sample)\n",
    "        pwm_sample = torch.unsqueeze(pwm_sample,dim=3)\n",
    "        return pwm_logits, pwm, pwm_sample\n",
    "    \n",
    "    def loss(self,):\n",
    "        pwm_logits, pwm, pwm_sample = self.forward()\n",
    "        x = pwm if self.training else pwm_sample\n",
    "        x = x[...,0].permute((0,2,1)).to(torch.float)\n",
    "        return self.predictor(x)\n",
    "\n",
    "    \n",
    "seq_length = 118\n",
    "original_seq = 'atcccgggtgaggcatcccaccatcctcagtcacagagagacccaatctaccatcagcatcagccagtaaagattaagaaaaacttagggtgaaagaaatttcacctaacacggcgca'\n",
    "original_seq=original_seq.upper()\n",
    "prefix,suffix = original_seq[:72],original_seq[96:]\n",
    "templates = [prefix+\"N\"*24+suffix] *10\n",
    "# templates = [\"N\"*seq_length] *10\n",
    "model = DNAPWM(10,seq_length)\n",
    "model.apply(MyInit(templates,seq_length=seq_length))\n",
    "criterion = MSELoss(target=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-06)\n",
    "# model.predictor.eval()\n",
    "x = model.loss()\n",
    "print(x)\n",
    "# x\n",
    "iterations = 10000\n",
    "epoch_loss=[]\n",
    "for it in tqdm.tqdm(range(iterations)):\n",
    "    model.train()\n",
    "    outputs = model.loss()\n",
    "    loss = criterion(outputs)\n",
    "    epoch_loss.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (it+1) % 1000 ==0:\n",
    "        loss = np.average(epoch_loss)\n",
    "        epoch_loss=[]\n",
    "        print(f\"Iterations:{it+1} Loss:{loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "_,pwm,seqs = model()\n",
    "# model.train()\n",
    "sample = torch.nn.functional.one_hot(torch.argmax(pwm[...,0],dim=2))\n",
    "sample=sample.permute((0,2,1)).to(torch.float)\n",
    "x= seqs[...,0].permute((0,2,1)).to(torch.float)\n",
    "chars = \"ACGT\"\n",
    "seqs_x = torch.argmax(pwm[...,0],dim=2)\n",
    "seqs = [\"\".join([chars[i] for i in seq]) for seq in seqs_x]\n",
    "dists = [Levenshtein.distance(original_seq, s) for s in seqs ]\n",
    "preds = model.predictor(sample).detach().numpy()[:,0]\n",
    "pd.DataFrame(data={\"seq\":seqs,\"distance\":dists,\"preds\":preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7608ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_seq = 'atcccgggtgaggcatcccaccatcctcagtcacagagagacccaatctaccatcagcatcagccagtaaagattaagaaaaacttagggtgaaagaaatttcacctaacacggcgca'\n",
    "original_seq=original_seq.upper()\n",
    "encoder = OneHotEncoder(118)\n",
    "x_test = torch.tensor(np.array([encoder(original_seq)]),dtype=torch.float)\n",
    "model.predictor.eval()\n",
    "model.predictor(torch.permute(x_test,(0,2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation = {}\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "  visualisation[m] = o \n",
    "\n",
    "def get_all_layers(net):\n",
    "  for name, layer in net._modules.items():\n",
    "    print(name)\n",
    "    #If it is a sequential, don't register a hook on it\n",
    "    # but recursively register hook on all it's module children\n",
    "    if isinstance(layer, nn.Sequential):\n",
    "      get_all_layers(layer)\n",
    "    else:\n",
    "      # it's a non sequential. Register a hook\n",
    "      layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# get_all_layers(model)\n",
    "# model.loss()\n",
    "# visualisation\n",
    "\n",
    "\n",
    "for name, para in model.named_parameters():\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

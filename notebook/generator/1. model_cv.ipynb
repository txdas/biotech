{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv1D,InputLayer,Convolution1D,MaxPooling1D,BatchNormalization,Concatenate,Input\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "seed=42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "def one_hot_encode(df, col='seq', seq_len=44):\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1], 'n':[0,0,0,0]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.empty([len(df),seq_len,4])\n",
    "    \n",
    "    # Iterate through UTRs and one-hot encode\n",
    "    for i,seq in enumerate(df[col].str[:seq_len]): \n",
    "        seq = seq.lower()\n",
    "        lst = [nuc_d[x] for x in seq]\n",
    "        if seq_len>len(seq):\n",
    "            lst += [nuc_d['n']]*(seq_len-len(seq))\n",
    "        a = np.array(lst)\n",
    "        vectors[i] = a\n",
    "    return vectors\n",
    "\n",
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def test_data(df, model, test_seq, obs_col, output_col='pred'):\n",
    "    '''Predict mean ribosome load using model and test set UTRs'''\n",
    "    \n",
    "    # Scale the test set mean ribosome load\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(df[obs_col].values.reshape(-1,1))\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_seq, verbose=0).reshape(-1,1)\n",
    "    \n",
    "    # Inverse scaled predicted mean ribosome load and return in a column labeled 'pred'\n",
    "    df.loc[:,output_col] = scaler.inverse_transform(predictions)\n",
    "    return df\n",
    "\n",
    "def get_cnn_model(seq_len=118,kernel_size=5, border_mode='same'):\n",
    "\n",
    "    ''' Build model archicture and fit.'''\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(name=\"conv_1\",activation=\"relu\", input_shape=(seq_len, 4), padding=border_mode, filters=100, kernel_size=kernel_size))\n",
    "    model.add(Conv1D(name=\"conv_2\",activation=\"relu\", padding=border_mode, filters=100, kernel_size=kernel_size))\n",
    "    model.add(Dropout(0.15,name=\"dropout_1\"))\n",
    "    model.add(Conv1D(name=\"conv_3\",activation=\"relu\", padding=border_mode, filters=100*2, kernel_size=kernel_size))\n",
    "    model.add(Dropout(0.15,name=\"dropout_2\"))\n",
    "    model.add(Flatten(name=\"flatten_1\"))\n",
    "    model.add(Dense(150,name=\"dense_1\",kernel_initializer='glorot_normal',bias_initializer='zeros'))\n",
    "    model.add(Activation('relu',name=\"act_1\"))\n",
    "    model.add(Dropout(0.15,name=\"dropout_out1\"))\n",
    "    model.add(Dense(1,name=\"dense_2\",kernel_initializer='glorot_normal',bias_initializer='zeros'))\n",
    "    model.add(Activation('linear',name=\"act_2\"))\n",
    "#     model.summary()\n",
    "    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-06)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "    return model\n",
    "\n",
    "def get_cnn1_model(seq_len=118,kernel_size=5, border_mode='same'):\n",
    "\n",
    "    ''' Build model archicture and fit.'''\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(name=\"conv_1\",activation=\"relu\", input_shape=(seq_len, 4), padding=border_mode, filters=50, kernel_size=kernel_size))\n",
    "    model.add(Conv1D(name=\"conv_2\",activation=\"relu\", padding=border_mode, filters=100, kernel_size=kernel_size))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2, padding=border_mode,name=\"maxpooling1\"))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Conv1D(name=\"conv_3\",activation=\"relu\", padding=border_mode, filters=200, kernel_size=kernel_size))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2, padding=border_mode,name=\"maxpooling2\"))\n",
    "    model.add(Conv1D(name=\"conv_4\",activation=\"relu\", padding=border_mode, filters=100, kernel_size=kernel_size))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100,name=\"dense_1\",kernel_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(1,name=\"dense_2\",activation='linear',kernel_initializer='glorot_normal'))\n",
    "#     model.summary()\n",
    "    #compile the model\n",
    "    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-06)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "    return model\n",
    "\n",
    "def get_cnn2_model(seq_len=118,kernel_size=5,conv_layers=3,reg_lambda=0.0):\n",
    "\n",
    "    ''' Build model archicture and fit.'''\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(seq_len, 4)))\n",
    "    conv_filters_first = 120\n",
    "    for conv_layer_idx in range(conv_layers):\n",
    "        model.add(Conv1D(activation=\"relu\",padding='same', filters=conv_filters_first*(2**0),kernel_size=kernel_size,\n",
    "                name='conv_{}_0'.format(conv_layer_idx + 1),kernel_regularizer=regularizers.l2(reg_lambda)))\n",
    "        model.add(Conv1D(activation=\"relu\",padding='same', filters=conv_filters_first*(2**conv_layer_idx),kernel_size=kernel_size,\n",
    "                name='conv_{}_1'.format(conv_layer_idx + 1),kernel_regularizer=regularizers.l2(reg_lambda)))\n",
    "        model.add(MaxPooling1D(pool_size=2,strides=2,padding='same'))\n",
    "        if conv_layer_idx:\n",
    "            model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50,name='dense_1',activation='relu',kernel_regularizer=regularizers.l2(reg_lambda)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1,name='dense_2',activation='linear',kernel_regularizer=regularizers.l2(reg_lambda)))\n",
    "#     model.summary()\n",
    "    #compile the model\n",
    "    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-06)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "    return model\n",
    "\n",
    "def train_model(model,e_train,e_test,seq_len=118):\n",
    "    # e_test = pd.concat([e_test,df[df.seq==original_seq]])\n",
    "    \n",
    "    label=\"score\" \n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=seq_len)\n",
    "    seq_e_test = one_hot_encode(e_test, seq_len=seq_len)\n",
    "    e_test.loc[:,'scaled_rl'] = preprocessing.StandardScaler().fit_transform(e_test.loc[:,label].values.reshape(-1,1))\n",
    "    e_train.loc[:,'scaled_rl'] = preprocessing.StandardScaler().fit_transform(e_train.loc[:,label].values.reshape(-1,1))\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='min', verbose=1, \n",
    "                          restore_best_weights=True)\n",
    "    set_seed(seed)\n",
    "    history = model.fit(seq_e_train, e_train['scaled_rl'], batch_size=64, epochs=50,callbacks=[earlyStop], \n",
    "                        validation_data=(seq_e_test,e_test[\"scaled_rl\"]), verbose=1,)\n",
    "    e_test = test_data(df=e_test, model=model, obs_col=label,test_seq=seq_e_test)\n",
    "    e_train = test_data(df=e_train, model=model, obs_col=label,test_seq=seq_e_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import Levenshtein\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold\n",
    "seed=42\n",
    "data_dir=\"/data/home/jinyalong/data/sev_240624/results\"\n",
    "original_seq = 'atcccgggtgaggcatcccaccatcctcagtcacagagagacccaatctaccatcagcatcagccagtaaagattaagaaaaacttagggtgaaagaaatttcacctaacacggcgca'\n",
    "original_seq=original_seq.upper()\n",
    "# lst = [f\"pl{i}-1-{j}\" for i in (3,5,6) for j in range(1,3)]\n",
    "# lst = [f\"pl{i}-1-{j}\" for i in (3,6) for j in range(1,3)]\n",
    "# dfs = []\n",
    "# for name in lst:\n",
    "#     cdf = pd.read_csv(f\"{data_dir}/{name}/{name}_final.csv\")\n",
    "#     dfs.append(cdf)\n",
    "# df = pd.concat(dfs)\n",
    "# seq_len=118\n",
    "# df = df.groupby(\"seq\")[[\"plasmid_counts\",\"rna_counts\"]].sum().reset_index()\n",
    "# df = df[(df[\"plasmid_counts\"]>40)&(df[\"rna_counts\"]>10)]\n",
    "# name,seq_len=\"pl3-1-2_core\" # 118,24\n",
    "# name,seq_len,suff = \"pl3-1-2\",118,\"\"\n",
    "name,seq_len,suff=\"SFV_0719\",85,\"\"\n",
    "# df = pd.read_csv(f\"{data_dir}/{name}/{name}_final.csv\")\n",
    "datadir=f\"/data/home/jinyalong/data/5UTR/{name}_\"\n",
    "# datadir=f\"/data/home/jinyalong/data/sev_240624/results/{name}/{name}_\"\n",
    "df = pd.read_csv(f\"{datadir}train{suff}.csv\")\n",
    "e_test= pd.read_csv(f\"{datadir}test{suff}.csv\")\n",
    "df['distance'] = df.seq.apply(lambda x: Levenshtein.distance(original_seq, x))\n",
    "distance_bins = np.arange(0, df.distance.max() // 5 * 4, df.distance.max() // 5)\n",
    "distance_bins = np.append(distance_bins, np.inf)\n",
    "df['distance_cat'] = pd.cut(df.distance, bins=distance_bins, labels=list(range(1, len(distance_bins))), right=False)\n",
    "df[\"length\"]=df[\"seq\"].apply(len)\n",
    "# frac = df[\"plasmid_counts\"].sum()/df[\"rna_counts\"].sum()\n",
    "# df[\"abs_score\"]=df[\"rna_counts\"]/df[\"plasmid_counts\"]\n",
    "# df[\"score\"]=np.log(df[\"abs_score\"]*frac)\n",
    "# print(df[df.seq==original_seq])\n",
    "# df = df[df.seq!=original_seq]\n",
    "# nums = [0]*(len(original_seq))\n",
    "# def edit_pos_cat(v):\n",
    "#     idxs = [i for i in range(len(original_seq)) if v[i]!=original_seq[i]] \n",
    "#     mval = min([nums[i] for i in idxs])\n",
    "#     hit = [i for i, n in enumerate(nums) if n==mval]\n",
    "#     nums[hit[0]]+=1\n",
    "#     return hit[0]\n",
    "\n",
    "\n",
    "# df[\"edit_pos\"] = df.seq.apply(edit_pos_cat)\n",
    "# # print(nums)\n",
    "# df[\"edit_pos\"]= df.edit_pos.apply(lambda x : x-1 if x==117 else x)\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=seed)\n",
    "rdf = df\n",
    "ns, ss = [], []\n",
    "for nsamples in range(500,14908,500):\n",
    "    df = rdf.sample(nsamples)\n",
    "    split = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    label=\"score\"\n",
    "    fpr = []\n",
    "    for idx,(train_idx, test_idx) in enumerate(split.split(df, df[\"length\"])):\n",
    "        e_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "#         e_test = df.iloc[test_idx].reset_index(drop=True)\n",
    "        if idx==0:\n",
    "            print(f\"Samples {nsamples} \", e_train.shape, e_test.shape)\n",
    "        elif idx==5:\n",
    "            break\n",
    "        #model = get_cnn_model(seq_len=seq_len,kernel_size=6)\n",
    "        model = get_cnn1_model(seq_len=seq_len,kernel_size=5)\n",
    "#         model = get_cnn2_model(seq_len=seq_len,kernel_size=6,reg_lambda=1e-4)\n",
    "        train_model(model,e_train,e_test,seq_len=seq_len)\n",
    "        model.save(f\"./models/tmp/{name}.keras\")\n",
    "        r = r2(e_test[label], e_test['pred'])\n",
    "        pr =  stats.pearsonr(e_test[label], e_test['pred'])\n",
    "        ns.append(nsamples)\n",
    "        ss.append(pr[0])\n",
    "        print(f'\\t KFold {idx} test r-squared = ', r, \"pearsonR = \", pr[0])\n",
    "    # scores.append(pr[0])\n",
    "#     e_train = test_data(df=e_train, model=model, obs_col=label,test_seq=seq_e_train)\n",
    "#     r = r2(e_train[label], e_train['pred'])\n",
    "#     pr =  stats.pearsonr(e_train[label], e_train['pred'])\n",
    "#     print(f'KFold {idx} train r-squared = ', r, \"pearsonR = \", pr[0])\n",
    "\n",
    "pdf = pd.DataFrame(data={\"samples\":ns,\"score\":ss})\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce74474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "name = name.split(\"_\")[0] if \"_\"in name else \"SEV\"\n",
    "name = \"SFV\"\n",
    "fig,ax=plt.subplots()\n",
    "# mx=pdf.groupby('samples')['score'].median()\n",
    "# sns.boxplot(y='score',x='samples',data=pdf,ax=ax)\n",
    "plot=sns.pointplot(data=pdf,x=\"samples\", y=\"score\",scale=0.5)\n",
    "# plot.set_xticklabels(plot.get_xticklabels(), rotation=75)\n",
    "plot.set_title(f\"{name}\")\n",
    "plot.set_xticks(ticks=range(0,35,3))\n",
    "# plot.set_xticklabels(, rotation=75)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

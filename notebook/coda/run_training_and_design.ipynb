{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836ac7b-3e52-4870-b396-583bf42aa7b1",
   "metadata": {},
   "source": [
    "# Modeling + Design\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Rand scripts back-to-back to ensure compatability. \n",
    "- This notebook automatically tries to grab the most recent artifact from `/tmp/model_artifacts_*`\n",
    "    - You may need to update this or hand copy the artifact between `train.py` and `generate.py`\n",
    "- Settings (mostly for training) are adjusted for fast testing and will not reproduce similar results to the paper. \n",
    "- You will need to modify hyperparameters yourself. An obvious change is to set `--min_epochs 60` and `--max_epochs 200`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a853f-cc97-477f-945c-44a43aab7cf1",
   "metadata": {},
   "source": [
    "## Deploy training\n",
    "Deposit model in `/tmp/`. The progress bar doesn't play nice with the notebook, so you'll have to scroll a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b3ccd4-ce31-4608-984c-758a2a99fec1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "--------------------------------------------------\n",
      "\n",
      "K562 | top cut value: 10.54, bottom cut value: -6.43\n",
      "HepG2 | top cut value: 9.78, bottom cut value: -5.74\n",
      "SKNSH | top cut value: 10.36, bottom cut value: -6.4\n",
      "\n",
      "Number of examples discarded from top: 0\n",
      "Number of examples discarded from bottom: 0\n",
      "\n",
      "Number of examples available: 666435\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Padding sequences... \n",
      "\n",
      "Creating train/val/test datasets with tokenized sequences... \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Number of examples in train: 1691500 (253.81%)\n",
      "Number of examples in val:   55841 (8.38%)\n",
      "Number of examples in test:  45330 (6.8%)\n",
      "\n",
      "Excluded from train: -1126236 (-168.99)%\n",
      "--------------------------------------------------\n",
      "Copying gs://tewhey-public-data/CODA_resources/my-model.epoch_5-step_19885.pkl...\n",
      "- [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
      "Operation completed over 1 objects/18.5 MiB.                                     \n",
      "Key conv1.conv.weight successfully matched\n",
      "Key conv1.conv.bias successfully matched\n",
      "Key conv1.bn_layer.weight successfully matched\n",
      "Key conv1.bn_layer.bias successfully matched\n",
      "Key conv1.bn_layer.running_mean successfully matched\n",
      "Key conv1.bn_layer.running_var successfully matched\n",
      "Key conv1.bn_layer.num_batches_tracked successfully matched\n",
      "Key conv2.conv.weight successfully matched\n",
      "Key conv2.conv.bias successfully matched\n",
      "Key conv2.bn_layer.weight successfully matched\n",
      "Key conv2.bn_layer.bias successfully matched\n",
      "Key conv2.bn_layer.running_mean successfully matched\n",
      "Key conv2.bn_layer.running_var successfully matched\n",
      "Key conv2.bn_layer.num_batches_tracked successfully matched\n",
      "Key conv3.conv.weight successfully matched\n",
      "Key conv3.conv.bias successfully matched\n",
      "Key conv3.bn_layer.weight successfully matched\n",
      "Key conv3.bn_layer.bias successfully matched\n",
      "Key conv3.bn_layer.running_mean successfully matched\n",
      "Key conv3.bn_layer.running_var successfully matched\n",
      "Key conv3.bn_layer.num_batches_tracked successfully matched\n",
      "Key linear1.linear.weight successfully matched\n",
      "Key linear1.linear.bias successfully matched\n",
      "Key linear1.bn_layer.weight successfully matched\n",
      "Key linear1.bn_layer.bias successfully matched\n",
      "Key linear1.bn_layer.running_mean successfully matched\n",
      "Key linear1.bn_layer.running_var successfully matched\n",
      "Key linear1.bn_layer.num_batches_tracked successfully matched\n",
      "Missing key in dict: branched.branched_layer_1.weight\n",
      "Missing key in dict: branched.branched_layer_1.bias\n",
      "Missing key in dict: branched.branched_layer_2.weight\n",
      "Missing key in dict: branched.branched_layer_2.bias\n",
      "Missing key in dict: branched.branched_layer_3.weight\n",
      "Missing key in dict: branched.branched_layer_3.bias\n",
      "Size mismatch for key: output.weight, expected size torch.Size([3, 140, 1]), got torch.Size([280, 1000])\n",
      "Size mismatch for key: output.bias, expected size torch.Size([3, 1, 1]), got torch.Size([280])\n",
      "Skipped loading key: linear2.linear.weight of size torch.Size([1000, 1000])\n",
      "Skipped loading key: linear2.linear.bias of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.weight of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.bias of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.running_mean of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.running_var of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.num_batches_tracked of size torch.Size([])\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Found 4107183 parameters\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | BassetBranched | 4.1 M \n",
      "1 | criterion | L1KLmixed      | 0     \n",
      "---------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "8.214     Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2917: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:01<00:00,  1.12it/s]/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:236: UserWarning: You called `self.log('current_epoch', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.12310 | harmonic_mean_loss: 0.99485 | prediction_mean_spearman: 0.03974 | entropy_spearman: 0.03086 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0:   0%|                                         | 0/1625 [00:00<?, ?it/s]starting epoch 0\n",
      "Epoch 0:  97%|████████▋| 1573/1625 [01:50<00:03, 14.22it/s, loss=0.105, v_num=8]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1574/1625 [01:51<00:03, 14.14it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1575/1625 [01:51<00:03, 14.14it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1576/1625 [01:51<00:03, 14.15it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1577/1625 [01:51<00:03, 14.16it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1578/1625 [01:51<00:03, 14.16it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1579/1625 [01:51<00:03, 14.17it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1580/1625 [01:51<00:03, 14.17it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1581/1625 [01:51<00:03, 14.18it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1582/1625 [01:51<00:03, 14.19it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1583/1625 [01:51<00:02, 14.19it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1584/1625 [01:51<00:02, 14.20it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1585/1625 [01:51<00:02, 14.21it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1586/1625 [01:51<00:02, 14.21it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1587/1625 [01:51<00:02, 14.22it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1588/1625 [01:51<00:02, 14.22it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1589/1625 [01:51<00:02, 14.23it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1590/1625 [01:51<00:02, 14.24it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1591/1625 [01:51<00:02, 14.24it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1592/1625 [01:51<00:02, 14.25it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1593/1625 [01:51<00:02, 14.26it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1594/1625 [01:51<00:02, 14.26it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1595/1625 [01:51<00:02, 14.27it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1596/1625 [01:51<00:02, 14.27it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1597/1625 [01:51<00:01, 14.28it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1598/1625 [01:51<00:01, 14.29it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1599/1625 [01:51<00:01, 14.29it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1600/1625 [01:51<00:01, 14.30it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1601/1625 [01:51<00:01, 14.30it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1602/1625 [01:51<00:01, 14.31it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1603/1625 [01:51<00:01, 14.32it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1604/1625 [01:51<00:01, 14.32it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1605/1625 [01:52<00:01, 14.33it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1606/1625 [01:52<00:01, 14.33it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1607/1625 [01:52<00:01, 14.34it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1608/1625 [01:52<00:01, 14.35it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1609/1625 [01:52<00:01, 14.35it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1610/1625 [01:52<00:01, 14.36it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1611/1625 [01:52<00:00, 14.36it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1612/1625 [01:52<00:00, 14.37it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1613/1625 [01:52<00:00, 14.38it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1614/1625 [01:52<00:00, 14.38it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1615/1625 [01:52<00:00, 14.39it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1616/1625 [01:52<00:00, 14.40it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1617/1625 [01:52<00:00, 14.40it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1618/1625 [01:52<00:00, 14.41it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1619/1625 [01:52<00:00, 14.41it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1620/1625 [01:52<00:00, 14.42it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1621/1625 [01:52<00:00, 14.43it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1622/1625 [01:52<00:00, 14.43it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1623/1625 [01:52<00:00, 14.44it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1624/1625 [01:52<00:00, 14.44it/s, loss=0.105, v_num=8]\u001b[A\n",
      "Epoch 0: 100%|█████████| 1625/1625 [01:52<00:00, 14.45it/s, loss=0.105, v_num=8]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.09195 | harmonic_mean_loss: 0.47953 | prediction_mean_spearman: 0.69418 | entropy_spearman: 0.38910 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0: 100%|█████████| 1625/1625 [01:52<00:00, 14.45it/s, loss=0.105, v_num=8]\n",
      "Epoch 1:   0%|                    | 0/1625 [00:00<?, ?it/s, loss=0.105, v_num=8]starting epoch 1\n",
      "Epoch 1:  97%|███████▋| 1573/1625 [01:50<00:03, 14.21it/s, loss=0.0891, v_num=8]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  97%|███████▋| 1574/1625 [01:51<00:03, 14.13it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1575/1625 [01:51<00:03, 14.13it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1576/1625 [01:51<00:03, 14.14it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1577/1625 [01:51<00:03, 14.15it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1578/1625 [01:51<00:03, 14.15it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1579/1625 [01:51<00:03, 14.16it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1580/1625 [01:51<00:03, 14.17it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1581/1625 [01:51<00:03, 14.17it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1582/1625 [01:51<00:03, 14.18it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1583/1625 [01:51<00:02, 14.18it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1584/1625 [01:51<00:02, 14.19it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1585/1625 [01:51<00:02, 14.20it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1586/1625 [01:51<00:02, 14.20it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1587/1625 [01:51<00:02, 14.21it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1588/1625 [01:51<00:02, 14.21it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1589/1625 [01:51<00:02, 14.22it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1590/1625 [01:51<00:02, 14.23it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1591/1625 [01:51<00:02, 14.23it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1592/1625 [01:51<00:02, 14.24it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1593/1625 [01:51<00:02, 14.25it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1594/1625 [01:51<00:02, 14.25it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1595/1625 [01:51<00:02, 14.26it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1596/1625 [01:51<00:02, 14.26it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1597/1625 [01:51<00:01, 14.27it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1598/1625 [01:51<00:01, 14.28it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1599/1625 [01:51<00:01, 14.28it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  98%|███████▉| 1600/1625 [01:51<00:01, 14.29it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1601/1625 [01:51<00:01, 14.30it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1602/1625 [01:52<00:01, 14.30it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1603/1625 [01:52<00:01, 14.31it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1604/1625 [01:52<00:01, 14.31it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1605/1625 [01:52<00:01, 14.32it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1606/1625 [01:52<00:01, 14.33it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1607/1625 [01:52<00:01, 14.33it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1608/1625 [01:52<00:01, 14.34it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1609/1625 [01:52<00:01, 14.34it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1610/1625 [01:52<00:01, 14.35it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1611/1625 [01:52<00:00, 14.36it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1612/1625 [01:52<00:00, 14.36it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1613/1625 [01:52<00:00, 14.37it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1614/1625 [01:52<00:00, 14.37it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1615/1625 [01:52<00:00, 14.38it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1616/1625 [01:52<00:00, 14.39it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1617/1625 [01:52<00:00, 14.39it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1618/1625 [01:52<00:00, 14.40it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1619/1625 [01:52<00:00, 14.41it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1620/1625 [01:52<00:00, 14.41it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1621/1625 [01:52<00:00, 14.42it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1622/1625 [01:52<00:00, 14.42it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1623/1625 [01:52<00:00, 14.43it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1624/1625 [01:52<00:00, 14.44it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "Epoch 1: 100%|████████| 1625/1625 [01:52<00:00, 14.44it/s, loss=0.0891, v_num=8]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | arithmetic_mean_loss: 0.08037 | harmonic_mean_loss: 0.37191 | prediction_mean_spearman: 0.76195 | entropy_spearman: 0.47222 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1: 100%|████████| 1625/1625 [01:52<00:00, 14.44it/s, loss=0.0891, v_num=8]\n",
      "Epoch 2:   0%|                   | 0/1625 [00:00<?, ?it/s, loss=0.0891, v_num=8]starting epoch 2\n",
      "Epoch 2:  97%|███████▋| 1573/1625 [01:50<00:03, 14.18it/s, loss=0.0975, v_num=8]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  97%|███████▋| 1574/1625 [01:51<00:03, 14.10it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1575/1625 [01:51<00:03, 14.11it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1576/1625 [01:51<00:03, 14.12it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1577/1625 [01:51<00:03, 14.12it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1578/1625 [01:51<00:03, 14.13it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1579/1625 [01:51<00:03, 14.13it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1580/1625 [01:51<00:03, 14.14it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1581/1625 [01:51<00:03, 14.15it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1582/1625 [01:51<00:03, 14.15it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1583/1625 [01:51<00:02, 14.16it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1584/1625 [01:51<00:02, 14.16it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1585/1625 [01:51<00:02, 14.17it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1586/1625 [01:51<00:02, 14.18it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1587/1625 [01:51<00:02, 14.18it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1588/1625 [01:51<00:02, 14.19it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1589/1625 [01:51<00:02, 14.20it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1590/1625 [01:51<00:02, 14.20it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1591/1625 [01:51<00:02, 14.21it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1592/1625 [01:52<00:02, 14.21it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1593/1625 [01:52<00:02, 14.22it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1594/1625 [01:52<00:02, 14.23it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1595/1625 [01:52<00:02, 14.23it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1596/1625 [01:52<00:02, 14.24it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1597/1625 [01:52<00:01, 14.24it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1598/1625 [01:52<00:01, 14.25it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1599/1625 [01:52<00:01, 14.26it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  98%|███████▉| 1600/1625 [01:52<00:01, 14.26it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1601/1625 [01:52<00:01, 14.27it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1602/1625 [01:52<00:01, 14.28it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1603/1625 [01:52<00:01, 14.28it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1604/1625 [01:52<00:01, 14.29it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1605/1625 [01:52<00:01, 14.29it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1606/1625 [01:52<00:01, 14.30it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1607/1625 [01:52<00:01, 14.31it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1608/1625 [01:52<00:01, 14.31it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1609/1625 [01:52<00:01, 14.32it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1610/1625 [01:52<00:01, 14.32it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1611/1625 [01:52<00:00, 14.33it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1612/1625 [01:52<00:00, 14.34it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1613/1625 [01:52<00:00, 14.34it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1614/1625 [01:52<00:00, 14.35it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1615/1625 [01:52<00:00, 14.35it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1616/1625 [01:52<00:00, 14.36it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1617/1625 [01:52<00:00, 14.37it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1618/1625 [01:52<00:00, 14.37it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1619/1625 [01:52<00:00, 14.38it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1620/1625 [01:52<00:00, 14.38it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1621/1625 [01:52<00:00, 14.39it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1622/1625 [01:52<00:00, 14.40it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1623/1625 [01:52<00:00, 14.40it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1624/1625 [01:52<00:00, 14.41it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "Epoch 2: 100%|████████| 1625/1625 [01:52<00:00, 14.41it/s, loss=0.0975, v_num=8]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | arithmetic_mean_loss: 0.08448 | harmonic_mean_loss: 0.43486 | prediction_mean_spearman: 0.72950 | entropy_spearman: 0.43513 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 2: 100%|████████| 1625/1625 [01:52<00:00, 14.41it/s, loss=0.0975, v_num=8]\n",
      "Epoch 2: 100%|████████| 1625/1625 [01:52<00:00, 14.41it/s, loss=0.0975, v_num=8]`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100%|████████| 1625/1625 [01:52<00:00, 14.41it/s, loss=0.0975, v_num=8]\n",
      "Best model stashed at: /tmp/output/artifacts/lightning_logs/version_8/checkpoints/epoch=1-step=3146.ckpt\n",
      "Exists: True\n",
      "Setting model from epoch: 1\n",
      "entropy_spearman at 4719: 0.47222232818603516\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/train.py \\\n",
    "  --data_module=MPRA_DataModule \\\n",
    "    --datafile_path=gs://tewhey-public-data/CODA_resources/MPRA_ALL_HD_v2.txt \\\n",
    "    --sep space --sequence_column nt_sequence \\\n",
    "    --activity_columns K562_mean HepG2_mean SKNSH_mean \\\n",
    "    --stderr_columns lfcSE_k562 lfcSE_hepg2 lfcSE_sknsh \\\n",
    "    --synth_val_pct=0.0 --synth_test_pct=99.98 \\\n",
    "    --batch_size=1076 --duplication_cutoff=0.5 --std_multiple_cut=6.0 \\\n",
    "    --val_chrs 7 13 --test_chrs 9 21 X \\\n",
    "    --padded_seq_len=600 --use_reverse_complements=True --num_workers=8 \\\n",
    "  --model_module=BassetBranched \\\n",
    "    --input_len 600 \\\n",
    "    --conv1_channels=300 --conv1_kernel_size=19 \\\n",
    "    --conv2_channels=200 --conv2_kernel_size=11 \\\n",
    "    --conv3_channels=200 --conv3_kernel_size=7 \\\n",
    "    --linear_activation=ReLU --linear_channels=1000 \\\n",
    "    --linear_dropout_p=0.11625456877954289 \\\n",
    "    --branched_activation=ReLU --branched_channels=140 \\\n",
    "    --branched_dropout_p=0.5757068086404574 \\\n",
    "    --n_outputs=3 --n_linear_layers=1 \\\n",
    "    --n_branched_layers=3 --n_branched_layers=3 \\\n",
    "    --use_batch_norm=True --use_weight_norm=False \\\n",
    "    --loss_criterion=L1KLmixed --beta=5.0 \\\n",
    "    --reduction=mean \\\n",
    "  --graph_module=CNNTransferLearning \\\n",
    "    --parent_weights=gs://tewhey-public-data/CODA_resources/my-model.epoch_5-step_19885.pkl \\\n",
    "    --frozen_epochs=0 \\\n",
    "    --optimizer=Adam --amsgrad=True \\\n",
    "    --lr=0.0032658700881052086 --eps=1e-08 --weight_decay=0.0003438210249762151 \\\n",
    "    --beta1=0.8661062881299633 --beta2=0.879223105336538 \\\n",
    "    --scheduler=CosineAnnealingWarmRestarts --scheduler_interval=step \\\n",
    "    --T_0=4096 --T_mult=1 --eta_min=0.0 --last_epoch=-1 \\\n",
    "    --checkpoint_monitor=entropy_spearman --stopping_mode=max \\\n",
    "    --stopping_patience=30 --accelerator=gpu --devices=1 --min_epochs=1 --max_epochs=3 \\\n",
    "    --precision=16 --default_root_dir=/tmp/output/artifacts \\\n",
    "    --artifact_path=/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c057c926-84dc-4c7d-bcd8-f86c8963c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_artifacts__20240216_170930__404251.tar.gz\n"
     ]
    }
   ],
   "source": [
    "files = !ls /tmp/\n",
    "\n",
    "_ = [ print(p) for p in files if 'model_artifacts' in p ]\n",
    "model_artifact = [ p for p in files if 'model_artifacts' in p ][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b97e1a-7a9e-4ecb-88dc-1cb50dd5cca5",
   "metadata": {},
   "source": [
    "## Design sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c3e6d-9a9e-41a5-bccc-82776a05dab1",
   "metadata": {},
   "source": [
    "### Fast SeqProp\n",
    "We collected the path of the model artifact generated in the last step and will pass it to bash using `--model_artifact /tmp/{model_artifact}`. Probably change this for your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03e2f42-e895-45bb-a380-86ae700eb8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive unpacked in /tmp/tmpeu9hu1zu\n",
      "Loaded model from 20240216_170930 in eval mode\n",
      "Starting round: 0, generate 1000 proposals\n",
      "Steps:   0%|                                            | 0/200 [00:00<?, ?it/s]Penalty not implemented\n",
      "Steps: 100%|█████████████| 200/200 [00:53<00:00,  3.70it/s, Loss=-5.32, LR=1e-6]\n",
      "Steps: 100%|█████████████| 200/200 [00:52<00:00,  3.81it/s, Loss=-5.33, LR=1e-6]\n",
      "Steps: 100%|█████████████| 200/200 [00:52<00:00,  3.82it/s, Loss=-5.33, LR=1e-6]\n",
      "Steps: 100%|██████████████| 200/200 [00:52<00:00,  3.83it/s, Loss=-5.3, LR=1e-6]\n",
      "finished round\n",
      "Proposals deposited at:\n",
      "\t/tmp/test__k562__fsp__20240216_171503__555961.pt\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/generate.py \\\n",
    "    --params_module StraightThroughParameters \\\n",
    "        --batch_size 256 --n_channels 4 \\\n",
    "        --length 200 --n_samples 10 \\\n",
    "        --use_norm True --use_affine False \\\n",
    "    --energy_module MinGapEnergy \\\n",
    "        --target_feature 0 --bending_factor 1.0 --a_min -2.0 --a_max 6.0 \\\n",
    "        --model_artifact /tmp/{model_artifact} \\\n",
    "    --generator_module FastSeqProp \\\n",
    "         --n_steps 200 --learning_rate 0.5 \\\n",
    "    --energy_threshold -0.5 --max_attempts 40 \\\n",
    "    --n_proposals 1000 \\\n",
    "    --proposal_path /tmp/test__k562__fsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79162b13-a515-4cca-8d42-a3c711c05544",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposals': [{'states': tensor([[[-3.7070, -3.2406, -3.8673,  ..., -0.4899, -2.4676,  4.9513],\n",
       "            [ 9.2152, 10.3142, -2.7897,  ..., -0.5026,  4.1187, -1.9516],\n",
       "            [-1.5741, -2.1167, -3.4816,  ...,  0.0254, -0.0922, -1.7666],\n",
       "            [-0.9142, -2.0754,  7.5319,  ...,  0.7825,  0.1338, -3.1252]],\n",
       "   \n",
       "           [[-4.5731, -2.7901, -3.8281,  ..., -2.0469, -2.8894,  9.8387],\n",
       "            [ 3.1643, 11.9951, -4.1636,  ..., -1.4933, -3.2741, -3.3825],\n",
       "            [ 7.4364, -3.3401, -4.8432,  ..., -5.3925, 10.0816, -4.7574],\n",
       "            [-1.1955, -2.1979,  9.3587,  ..., 10.6572, -5.7531, -2.0952]],\n",
       "   \n",
       "           [[-4.0923, -3.3490, -3.7582,  ..., -0.1841, -2.2370,  3.1349],\n",
       "            [10.1623, 12.5539, -2.6694,  ..., -4.5843, 10.3012, -0.1835],\n",
       "            [-2.1776, -2.9983, -3.7627,  ...,  4.0314, -2.8292,  0.5169],\n",
       "            [ 0.4006, -1.6453,  9.8911,  ..., -0.3800, -1.7853, -3.3947]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-3.6596, -3.1684, -3.7423,  ...,  0.0442, -1.6888,  3.4881],\n",
       "            [-2.7194,  8.8432, -4.6518,  ...,  0.2198,  2.9509, -0.3736],\n",
       "            [14.5129, -3.5573, -2.6124,  ..., -2.4674,  1.9612, -2.6731],\n",
       "            [-2.8263, -3.2439,  8.0494,  ...,  1.1743, -1.2285, -1.2169]],\n",
       "   \n",
       "           [[-4.1268, -3.3374, -3.5045,  ..., -0.8683, -2.4045, -3.3308],\n",
       "            [ 1.5278,  6.9082, -3.3191,  ...,  5.2650,  6.5763,  6.2287],\n",
       "            [ 5.8422, -2.8773, -3.6461,  ..., -2.2503, -1.6921, -3.3335],\n",
       "            [-1.6599, -1.8684,  6.0976,  ..., -1.8545, -1.2756, -0.1734]],\n",
       "   \n",
       "           [[-4.2430, -3.0118, -3.4953,  ..., -4.0787,  4.5410,  2.9703],\n",
       "            [ 4.9565,  9.9210, -2.3413,  ..., -0.8391, -2.7223, -1.7288],\n",
       "            [ 2.5938, -2.3352, -3.7037,  ..., 13.3776, -0.5056,  2.7528],\n",
       "            [-0.8045, -1.7035,  7.3499,  ..., -3.7421, -4.5113, -4.3609]]]),\n",
       "   'proposals': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "            [0., 1., 1.,  ..., 0., 0., 1.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "            [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "            [0., 1., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 1.],\n",
       "            [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.]]]),\n",
       "   'energies': tensor([-6.8630, -6.4040, -6.2491, -6.6263, -6.1709, -6.9687, -7.1378, -7.5016,\n",
       "           -6.1937, -6.3819, -5.7842, -6.3103, -5.9286, -6.4435, -6.3732, -6.5321,\n",
       "           -6.3642, -7.2430, -6.2676, -6.7628, -7.3582, -7.3159, -6.2324, -6.5842,\n",
       "           -6.2242, -6.9842, -7.2251, -6.2944, -6.7684, -5.7780, -6.5118, -6.7253,\n",
       "           -5.8155, -6.0545, -6.3240, -6.7135, -7.0379, -6.7654, -7.0862, -6.5158,\n",
       "           -6.4815, -6.6931, -5.9294, -7.7271, -5.7058, -7.0980, -6.6277, -6.4191,\n",
       "           -6.6501, -6.1759, -6.5359, -6.0713, -6.5497, -7.2487, -6.4454, -6.5767,\n",
       "           -6.2595, -6.6081, -7.1559, -6.5353, -7.1783, -6.7199, -6.3837, -6.8237,\n",
       "           -6.9453, -6.1769, -6.7264, -6.5668, -5.8742, -6.5409, -6.0050, -6.2505,\n",
       "           -6.3038, -5.8011, -6.9697, -5.2656, -5.7684, -6.8199, -6.5443, -6.9258,\n",
       "           -6.3929, -6.6231, -5.5208, -5.9944, -5.9380, -6.5118, -7.4026, -7.1582,\n",
       "           -6.1506, -6.6345, -6.2889, -6.7478, -6.4956, -6.3728, -6.4968, -6.7206,\n",
       "           -6.9475, -7.2317, -5.7685, -6.3222, -6.6126, -6.5467, -6.3210, -6.9003,\n",
       "           -6.0160, -6.5051, -6.7346, -7.1276, -5.6422, -6.3968, -6.0360, -6.8952,\n",
       "           -5.8110, -6.5338, -5.4313, -6.1014, -7.1363, -6.1518, -6.8615, -7.5760,\n",
       "           -5.4710, -6.9572, -7.3984, -6.3864, -7.2066, -6.4924, -6.3249, -6.0347,\n",
       "           -6.6472, -6.5092, -6.1255, -6.1704, -7.2696, -6.9298, -6.0996, -6.7344,\n",
       "           -5.8305, -7.0993, -6.7065, -6.8923, -6.3646, -6.3181, -6.5091, -6.8130,\n",
       "           -6.7543, -6.2551, -6.4510, -6.3916, -6.6871, -6.3235, -6.6983, -6.3616,\n",
       "           -6.7427, -6.2692, -6.4115, -6.8453, -6.2759, -7.0792, -6.8897, -6.6671,\n",
       "           -6.6543, -6.3573, -7.6920, -6.2408, -6.1602, -6.0137, -5.9359, -6.3068,\n",
       "           -6.5120, -5.9361, -6.4140, -6.4469, -6.2052, -6.6480, -6.4528, -6.2984,\n",
       "           -6.4678, -6.4726, -6.0759, -6.5868, -7.2305, -5.1562, -6.5100, -6.6032,\n",
       "           -6.6529, -6.5677, -6.6509, -6.3022, -5.9908, -6.9654, -6.3855, -7.1521,\n",
       "           -5.4616, -6.2749, -6.6776, -6.7613, -5.8848, -7.4196, -5.2981, -6.3731,\n",
       "           -6.9305, -6.7396, -6.6997, -7.2301, -6.5503, -6.3255, -6.5234, -6.5221,\n",
       "           -6.9229, -6.5896, -6.8742, -5.5106, -6.9402, -6.4023, -6.3502, -6.4063,\n",
       "           -6.5459, -5.3208, -6.2285, -6.3626, -6.4980, -6.4842, -5.9536, -6.0353,\n",
       "           -5.7252, -6.2057, -5.6851, -6.6151, -7.0802, -6.4258, -5.9098, -6.3399,\n",
       "           -6.7819, -5.8621, -6.7292, -6.7972, -6.3300, -6.0661, -6.8937, -6.0829,\n",
       "           -7.0278, -5.9961, -7.2824, -6.3474, -6.6236, -6.2010, -6.1745, -6.1788,\n",
       "           -6.3163, -5.6834, -7.1301, -6.5680, -5.9417, -6.2531, -6.1332, -6.6849,\n",
       "           -6.2286, -6.7033, -6.3536, -5.7680, -6.3329, -6.6774, -6.7674, -6.2483,\n",
       "           -7.2952, -7.1510, -6.2410, -6.2336, -6.2411, -6.2502, -5.8846, -6.7764,\n",
       "           -6.1379, -6.4845, -7.0010, -6.6060, -6.0933, -6.8404, -6.6708, -6.5593,\n",
       "           -6.1981, -7.1202, -6.4816, -6.5607, -6.6923, -6.3513, -6.9968, -6.7313,\n",
       "           -6.6380, -6.6115, -6.6426, -6.4415, -6.5416, -6.2301, -6.9830, -5.9879,\n",
       "           -6.7870, -7.5116, -6.6586, -6.2549, -6.6573, -6.3781, -5.8381, -5.9318,\n",
       "           -6.6632, -5.9607, -5.9361, -6.4148, -6.5567, -6.1159, -6.8207, -6.3096,\n",
       "           -6.3832, -5.7369, -6.7379, -7.1828, -6.9266, -6.6616, -6.5739, -5.7289,\n",
       "           -5.9434, -5.7756, -6.5373, -6.6265, -5.8809, -7.3261, -7.0610, -6.4106,\n",
       "           -6.6022, -6.7588, -6.9161, -6.3627, -6.5493, -6.2101, -7.1574, -6.3409,\n",
       "           -5.6913, -5.5648, -6.2457, -6.4698, -7.2408, -6.6472, -6.2958, -7.0577,\n",
       "           -7.1625, -7.0315, -7.3118, -6.7904, -6.7514, -7.0724, -6.2066, -6.1719,\n",
       "           -6.5908, -6.0936, -6.3980, -6.8046, -7.3065, -6.3508, -6.0884, -6.5606,\n",
       "           -6.6855, -6.0933, -7.1449, -6.8910, -7.1123, -6.0505, -6.0816, -6.5683,\n",
       "           -6.0497, -7.2850, -6.6648, -7.0548, -7.1237, -6.7473, -6.1850, -6.0209,\n",
       "           -6.0744, -6.9712, -6.2627, -6.1017, -6.4313, -7.1565, -6.0664, -6.9170,\n",
       "           -7.0492, -6.2001, -6.2608, -7.1181, -6.9784, -7.2320, -6.9424, -6.4245,\n",
       "           -7.0226, -7.3681, -5.8328, -6.8367, -6.3690, -6.3544, -7.0477, -6.6152,\n",
       "           -6.6418, -6.2350, -7.3309, -6.2877, -6.0509, -6.7453, -6.4728, -6.6830,\n",
       "           -6.8250, -6.5110, -6.3653, -7.1034, -7.0726, -6.7403, -6.2937, -6.1894,\n",
       "           -6.6845, -5.9083, -6.3844, -7.1634, -6.9289, -6.8782, -6.4689, -6.8812,\n",
       "           -6.3102, -6.9988, -7.1637, -6.0781, -6.7999, -6.3105, -6.2869, -6.6411,\n",
       "           -6.7542, -7.2771, -6.2653, -6.7858, -7.3460, -5.9040, -6.2428, -6.5055,\n",
       "           -6.2181, -6.5596, -6.7011, -6.1953, -6.2062, -6.1912, -6.7777, -6.4635,\n",
       "           -7.1075, -6.1987, -6.7609, -6.2018, -6.2387, -6.3933, -7.0150, -6.2559,\n",
       "           -6.0867, -5.9980, -7.0234, -6.3259, -6.0717, -6.9720, -6.2322, -7.2369,\n",
       "           -7.0230, -6.0734, -5.9201, -6.8710, -6.2921, -6.6411, -6.5313, -6.3266,\n",
       "           -6.2208, -6.6453, -6.6835, -6.2921, -5.6238, -6.9289, -6.2351, -7.3476,\n",
       "           -6.3424, -6.7145, -6.3738, -5.6512, -5.4253, -6.8048, -6.6179, -6.0228,\n",
       "           -6.0438, -5.8390, -6.2188, -6.5744, -5.9233, -6.7853, -6.7053, -6.6480,\n",
       "           -6.0073, -6.0870, -6.5621, -7.0925, -6.6206, -6.0592, -5.3806, -6.6904,\n",
       "           -6.7019, -6.6806, -6.4395, -7.7646, -6.8917, -6.7971, -6.4620, -6.9435,\n",
       "           -6.9853, -7.0942, -7.2102, -6.3927, -6.9731, -6.5053, -6.9617, -5.7276,\n",
       "           -6.1533, -5.9640, -5.7316, -6.5330, -6.9193, -5.9639, -6.0414, -6.4092,\n",
       "           -6.9091, -6.7432, -6.2715, -6.6166, -7.2039, -7.0221, -6.2786, -6.7673,\n",
       "           -6.6426, -6.4946, -6.4322, -6.1154, -6.2750, -6.3302, -6.7993, -6.8031,\n",
       "           -6.4916, -6.1835, -6.7133, -7.1549, -6.2331, -5.9161, -6.5908, -7.0936,\n",
       "           -6.8072, -6.6790, -6.1919, -6.4131, -6.2968, -6.2230, -6.7419, -6.8328,\n",
       "           -6.2952, -6.8776, -6.1419, -6.3059, -6.5017, -6.8632, -7.2971, -6.1738,\n",
       "           -6.2470, -6.1898, -6.4661, -5.6274, -6.0422, -6.3731, -6.4051, -6.4811,\n",
       "           -6.8596, -6.3973, -7.0742, -7.0634, -6.1494, -6.7537, -6.3628, -6.6768,\n",
       "           -6.2782, -5.7663, -6.5583, -6.2752, -7.4134, -6.4811, -6.3729, -6.8008,\n",
       "           -6.3263, -7.3931, -5.6448, -6.7832, -6.8452, -5.6235, -6.9033, -6.4528,\n",
       "           -6.5509, -6.4696, -7.1679, -6.3763, -6.4207, -6.2984, -5.6152, -6.0573,\n",
       "           -6.6556, -6.2812, -6.3430, -6.2770, -6.6181, -6.3573, -6.8029, -7.1113,\n",
       "           -5.7760, -6.7084, -5.8549, -6.3031, -6.0454, -5.9586, -7.2295, -6.5271,\n",
       "           -6.6045, -6.5440, -6.7087, -6.8083, -7.0610, -6.1154, -6.2330, -6.1846,\n",
       "           -6.5455, -5.8165, -6.5996, -6.9433, -6.3721, -6.9723, -6.2372, -6.3422,\n",
       "           -6.9683, -7.0207, -6.7302, -5.8029, -5.7301, -6.4613, -6.6245, -6.8989,\n",
       "           -6.2164, -6.5734, -6.1528, -5.8461, -6.8513, -5.7710, -6.6926, -7.1529,\n",
       "           -6.1431, -6.7951, -6.4761, -6.5173, -6.3810, -6.9501, -7.3141, -6.1031,\n",
       "           -6.5117, -5.8276, -7.1863, -6.4357, -6.7312, -6.4509, -6.8135, -5.5979,\n",
       "           -5.8251, -5.6441, -6.5864, -6.7351, -7.3864, -6.7829, -6.2667, -7.3159,\n",
       "           -7.2549, -6.8905, -5.5386, -7.2679, -6.7149, -6.3544, -6.3691, -6.6263,\n",
       "           -6.6226, -5.0400, -6.8120, -6.1892, -6.7381, -6.0007, -6.7536, -5.9788,\n",
       "           -6.4596, -6.1088, -6.0522, -6.1560, -5.9961, -6.6443, -6.5522, -6.2412,\n",
       "           -6.6177, -6.3423, -6.6819, -6.4036, -6.5936, -6.4549, -6.4805, -6.5162,\n",
       "           -7.2157, -6.8062, -6.5236, -6.2431, -6.4515, -7.0572, -6.4667, -7.3297,\n",
       "           -6.6666, -6.0308, -6.2606, -6.4369, -6.7901, -6.5045, -6.9823, -5.6613,\n",
       "           -6.4687, -6.0260, -6.4180, -6.2404, -6.7301, -6.3851, -7.1049, -6.0007,\n",
       "           -6.5274, -6.3348, -6.6504, -6.4476, -6.0418, -5.6984, -7.1767, -6.4493,\n",
       "           -6.6382, -6.2815, -6.1636, -6.7069, -7.0101, -6.1111, -6.1218, -6.8891,\n",
       "           -6.6311, -5.4196, -6.9887, -6.6299, -6.2503, -6.8426, -6.6511, -6.5895,\n",
       "           -7.0773, -7.4743, -7.4973, -7.0000, -5.9214, -6.3024, -6.3597, -6.6790,\n",
       "           -7.0582, -6.9839, -6.2723, -6.4720, -7.3323, -6.3788, -6.2059, -6.5316,\n",
       "           -6.4809, -6.2435, -6.0827, -6.5381, -6.6673, -7.0755, -6.5338, -7.1434,\n",
       "           -6.5048, -6.2891, -6.1010, -7.0336, -7.3400, -6.4046, -6.7421, -5.2763,\n",
       "           -6.3553, -6.4908, -7.0183, -7.1676, -6.7638, -6.1301, -6.6014, -6.7123,\n",
       "           -5.9742, -6.3390, -6.2912, -6.3270, -7.2093, -6.6753, -6.9744, -6.3808,\n",
       "           -6.3580, -6.8480, -6.9131, -6.7061, -6.4040, -6.7822, -6.3191, -6.7266,\n",
       "           -6.7664, -6.8999, -6.0779, -6.5667, -7.0824, -5.3645, -7.0196, -5.7381,\n",
       "           -6.8773, -6.4261, -6.6112, -6.4677, -6.7898, -6.1646, -6.8671, -7.0524,\n",
       "           -6.2275, -6.1098, -6.8028, -6.3351, -6.5734, -7.2659, -6.3455, -6.6622,\n",
       "           -6.7065, -5.7286, -6.1003, -6.8276, -7.0511, -6.4474, -6.2620, -6.4851,\n",
       "           -7.0321, -6.9790, -7.1071, -6.3835, -7.4231, -6.1770, -7.0046, -6.8183,\n",
       "           -6.6270, -6.3165, -6.2816, -5.8703, -7.4069, -6.6582, -6.0525, -6.0406,\n",
       "           -6.4772, -5.8871, -6.3757, -6.6517, -6.7279, -5.7041, -7.1814, -6.9082,\n",
       "           -6.2206, -6.8225, -6.5685, -7.0973, -6.4031, -6.8734, -6.6316, -6.6252,\n",
       "           -6.1615, -6.5075, -6.3587, -5.8600, -6.8947, -6.3302, -6.7476, -6.9260,\n",
       "           -5.8031, -6.7133, -6.4741, -6.2294, -6.1990, -6.5222, -6.6782, -5.6773,\n",
       "           -6.7741, -6.3021, -5.8773, -6.3532, -6.7452, -6.4894, -6.0496, -6.9119,\n",
       "           -6.2470, -6.4892, -5.3185, -7.5058, -7.2185, -5.5919, -6.7241, -6.5049,\n",
       "           -5.7482, -5.7364, -6.2791, -5.7964, -7.4177, -6.3280, -6.1951, -6.2369,\n",
       "           -6.2899, -6.4536, -5.6799, -6.5867, -6.8854, -7.7047, -6.8040, -6.0568,\n",
       "           -7.0218, -6.5911, -6.0868, -6.5518, -7.2235, -5.7223, -6.5990, -6.3365,\n",
       "           -7.5642, -6.4823, -6.2249, -6.3618, -6.6109, -6.0839, -7.0378, -6.1887,\n",
       "           -6.5471, -6.3025, -6.5617, -6.2687, -6.5841, -5.3339, -5.7327, -6.5532,\n",
       "           -6.1708, -6.3378, -6.5127, -6.6644, -6.9954, -6.7347, -6.5836, -6.5965,\n",
       "           -6.9939, -6.0018, -5.4509, -6.0766, -6.7512, -6.7620, -6.9779, -6.3515,\n",
       "           -7.2572, -6.6452, -6.1454, -5.8798, -6.8780, -5.8719, -6.9311, -6.9808,\n",
       "           -6.4809, -6.5511, -6.2715, -5.8697, -6.9590, -6.5496, -6.8885, -5.9777,\n",
       "           -6.5736, -6.8372, -6.3796, -7.0463, -7.0144, -6.4273, -6.1566, -6.2022,\n",
       "           -6.9463, -7.0255, -6.4717, -5.8047, -6.2996, -6.7289, -5.9789, -6.8363]),\n",
       "   'acceptance_rate': tensor(1.),\n",
       "   'penalty': None}],\n",
       " 'args': {'positional arguments': Namespace(),\n",
       "  'optional arguments': Namespace(help=None),\n",
       "  'Main args': Namespace(energy_module='MinGapEnergy', energy_threshold=-0.5, generator_module='FastSeqProp', max_attempts=40, monitor=None, n_proposals=[1000], params_module='StraightThroughParameters', penalty_module=None, proposal_path='/tmp/test__k562__fsp', reset_params=True, tolerate_unknown_args=False),\n",
       "  'Params Module args': Namespace(batch_dim=0, batch_size=256, cat_axis=-1, init_seqs=None, left_flank='GTACGGGAGGTATTGGACAGGCCGCAATAAAATATCTTTATTTTCATTACATCTGTGTGTTGGTTTTTTGTGTGAATCGATAGTACTAACATACGCTCTCCATCAAAACAAAACGAAACAAAACAAACTAGCAAAATAGGCTGTCCCCAGTGCAAGTGCAGGTGCCAGAACATTTCTCTGGCCTAACTGGCCGCTTGACG', length=200, n_channels=4, n_samples=10, right_flank='CACTGCGGCTCCTGCGATCTAACTGGCCGGTACCTGAGCTCGCTAGCCTCGAGGATATCAAGATCTGGCCTCGGCGGCCAAGCTTAGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTTGGTAAAGCCACCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATC', token_dim=-2, use_affine=False, use_norm=True),\n",
       "  'Energy Module args': Namespace(a_max=6.0, a_min=-2.0, bending_factor=1.0, model_artifact='/tmp/model_artifacts__20240216_170930__404251.tar.gz', target_alpha=1.0, target_feature=0),\n",
       "  'Generator Constructor args': Namespace(),\n",
       "  'Generator Runtime args': Namespace(learning_rate=0.5, lr_scheduler=True, n_steps=200, step_print=10)},\n",
       " 'timestamp': '20240216_171503',\n",
       " 'random_tag': 555961}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "fsp_props = !ls /tmp/\n",
    "fsp_props = [ p for p in fsp_props if 'test__k562__fsp' in p ][-1]\n",
    "torch.load(f'/tmp/{fsp_props}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28160bb-4aa0-407c-89e4-d18cec4b8612",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a021cf1-caa0-4171-b226-1861c6942548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive unpacked in /tmp/tmpuya_wra9\n",
      "Loaded model from 20240216_170930 in eval mode\n",
      "Starting round: 0, generate 1000 proposals\n",
      "collect samples\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]Penalty not implemented\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:34<00:00, 58.25it/s]\n",
      "attempt 1 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 63.05it/s]\n",
      "attempt 2 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 63.12it/s]\n",
      "attempt 3 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 63.01it/s]\n",
      "attempt 4 acceptance rate: 256/256\n",
      "finished round\n",
      "Proposals deposited at:\n",
      "\t/tmp/test__k562__sa__20240216_171729__147400.pt\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/generate.py \\\n",
    "    --params_module BasicParameters \\\n",
    "        --batch_size 256 --n_channels 4 \\\n",
    "        --length 200 \\\n",
    "    --energy_module MinGapEnergy \\\n",
    "        --target_feature 0 --bending_factor 0.0 --a_min -2.0 --a_max 6.0 \\\n",
    "        --model_artifact /tmp/{model_artifact} \\\n",
    "    --generator_module SimulatedAnnealing \\\n",
    "         --n_steps 2000 --n_positions 5 \\\n",
    "         --a 1.0 --b 1.0 --gamma 0.501 \\\n",
    "    --energy_threshold -0.5 --max_attempts 40 \\\n",
    "    --n_proposals 1000 \\\n",
    "    --proposal_path /tmp/test__k562__sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4a0b4f-4a19-4cb1-9d3b-012c7dd6a04d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposals': [{'proposals': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 1., 1.,  ..., 1., 0., 0.]],\n",
       "   \n",
       "           [[1., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[1., 0., 0.,  ..., 1., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[1., 0., 1.,  ..., 0., 0., 1.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 1., 1., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 1., 0.,  ..., 1., 0., 1.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 1., 0., 1.]]]),\n",
       "   'energies': tensor([-6.1970, -6.3643, -6.1347, -6.1311, -6.1658, -6.3336, -6.2983, -6.3424,\n",
       "           -6.1930, -6.3564, -6.1125, -6.2392, -6.2227, -5.9252, -6.1842, -6.3370,\n",
       "           -6.3392, -5.8654, -6.1850, -5.9858, -6.0748, -6.3209, -6.2769, -6.0889,\n",
       "           -6.1742, -6.0378, -5.8583, -6.2909, -5.9390, -6.2924, -5.8965, -5.8855,\n",
       "           -5.8755, -5.9983, -6.1721, -5.9639, -6.3665, -6.4019, -6.2315, -5.7466,\n",
       "           -5.7609, -6.3118, -6.1023, -6.1516, -5.9182, -6.1531, -6.2907, -6.2990,\n",
       "           -6.0230, -5.9177, -5.7314, -5.8534, -6.2034, -5.6282, -5.9879, -6.1142,\n",
       "           -6.3656, -6.4401, -6.3193, -6.3236, -6.0895, -6.0872, -6.1618, -6.3334,\n",
       "           -6.0063, -6.0301, -6.0984, -6.1917, -6.3272, -6.4827, -6.3315, -6.0716,\n",
       "           -6.3618, -5.9448, -5.9467, -5.9701, -5.7939, -6.1238, -6.3735, -6.2708,\n",
       "           -5.6628, -5.9993, -5.9750, -6.1575, -6.1412, -6.1923, -6.1292, -6.2783,\n",
       "           -6.2295, -6.2472, -5.2242, -6.3787, -6.2500, -6.1929, -6.3677, -5.8231,\n",
       "           -6.0166, -5.8598, -6.1547, -6.1984, -6.2486, -6.2005, -6.3016, -6.0992,\n",
       "           -6.2852, -5.9034, -6.3215, -6.1104, -6.3119, -6.1455, -6.4212, -5.9734,\n",
       "           -5.8782, -5.8559, -6.1211, -5.7881, -6.4272, -6.0899, -6.0285, -6.0276,\n",
       "           -5.6991, -6.0835, -5.8908, -6.2450, -6.1215, -6.3152, -5.8731, -6.4282,\n",
       "           -6.1641, -6.0826, -5.9029, -6.2209, -6.2511, -5.9141, -5.8138, -6.2402,\n",
       "           -6.2670, -6.3174, -5.7531, -6.0136, -6.3156, -6.0690, -5.9705, -6.1316,\n",
       "           -6.1164, -6.3202, -6.3088, -5.9845, -6.2036, -6.1557, -6.2488, -6.1571,\n",
       "           -6.1766, -6.1115, -6.1890, -6.2284, -6.3559, -6.3022, -5.8546, -6.0980,\n",
       "           -6.3712, -6.2012, -6.0678, -6.1544, -5.7624, -5.9827, -6.0644, -6.2734,\n",
       "           -6.0887, -6.0901, -6.2297, -5.9690, -6.1032, -6.0102, -6.2315, -6.2275,\n",
       "           -6.4555, -6.0614, -6.1088, -6.0327, -6.1392, -6.0252, -6.3183, -6.1270,\n",
       "           -5.9348, -6.2876, -6.1217, -5.8707, -5.9893, -6.4717, -5.8623, -5.8126,\n",
       "           -6.0779, -6.0667, -6.0082, -6.3024, -6.0862, -5.9589, -6.3283, -5.8968,\n",
       "           -6.2358, -6.4138, -6.2227, -6.2615, -6.1959, -6.4137, -6.2995, -6.2310,\n",
       "           -6.2617, -6.3338, -5.8493, -5.5890, -5.6001, -6.2688, -6.2316, -6.2810,\n",
       "           -6.4495, -5.8445, -6.0556, -5.8607, -6.1472, -6.0528, -6.1491, -6.3187,\n",
       "           -6.2455, -6.1801, -6.1922, -6.0856, -5.8602, -6.1004, -5.9008, -6.1693,\n",
       "           -6.2329, -6.0646, -6.1939, -6.0998, -6.2000, -6.1847, -6.1177, -6.1722,\n",
       "           -6.0690, -6.1634, -6.0549, -6.0206, -6.2197, -5.9280, -6.2877, -6.0388,\n",
       "           -5.9828, -6.1288, -6.1629, -6.2126, -6.2160, -6.0786, -6.1660, -6.1836,\n",
       "           -5.8730, -6.3917, -6.2649, -6.2096, -6.1396, -6.1672, -6.1506, -6.2271,\n",
       "           -5.9711, -6.0681, -6.2523, -6.2119, -6.0775, -6.2514, -6.4273, -6.1496,\n",
       "           -5.8665, -6.2027, -6.1462, -6.2882, -6.2377, -6.0628, -5.9943, -5.9679,\n",
       "           -6.1312, -6.3776, -6.2651, -5.9713, -6.1390, -6.3066, -6.0807, -5.7750,\n",
       "           -6.3645, -6.1207, -6.1730, -5.7749, -6.1243, -6.3689, -6.0074, -6.3095,\n",
       "           -6.2267, -6.0743, -6.0725, -6.2593, -6.0381, -6.4033, -6.4681, -6.3025,\n",
       "           -6.2119, -6.1726, -6.2851, -6.2758, -6.1882, -6.1698, -5.9989, -6.1694,\n",
       "           -6.1086, -6.1959, -5.9356, -6.2791, -5.9530, -6.2365, -6.2756, -6.3075,\n",
       "           -6.0784, -6.2790, -6.1584, -6.3995, -6.0110, -6.2336, -6.1314, -6.3764,\n",
       "           -6.2008, -6.0850, -6.2712, -6.0246, -6.1421, -5.8276, -6.0525, -6.1749,\n",
       "           -5.9918, -5.7628, -5.9728, -6.1295, -5.9645, -6.4871, -6.3116, -5.9662,\n",
       "           -6.2528, -6.1150, -5.7539, -6.1119, -6.3376, -5.7488, -6.2534, -6.0449,\n",
       "           -6.2555, -5.6014, -6.0235, -5.7857, -6.0813, -5.7922, -6.4344, -6.1722,\n",
       "           -6.2187, -6.2991, -6.3130, -6.1731, -5.8234, -6.1181, -5.7247, -6.2488,\n",
       "           -6.3974, -6.4183, -6.1961, -5.8745, -6.1563, -6.0083, -5.9304, -6.3755,\n",
       "           -6.0961, -6.2228, -6.1774, -6.3996, -6.5463, -6.3797, -6.2014, -6.0429,\n",
       "           -6.2663, -6.2324, -5.9470, -6.2346, -6.3841, -6.1987, -6.2105, -6.0667,\n",
       "           -6.1511, -6.1436, -6.2783, -6.0487, -6.3386, -6.2474, -6.2240, -6.1016,\n",
       "           -5.9027, -6.4430, -6.0236, -5.9413, -6.2869, -5.9531, -5.8799, -6.4053,\n",
       "           -6.3309, -6.3059, -6.2229, -6.0440, -6.2287, -6.0903, -6.1270, -5.5771,\n",
       "           -6.3499, -6.0185, -6.2427, -6.4486, -6.2645, -5.7389, -6.0586, -6.0766,\n",
       "           -5.7030, -6.2573, -6.1757, -6.3322, -5.9903, -6.1371, -6.3266, -6.1756,\n",
       "           -6.0473, -5.6745, -6.3724, -6.1897, -6.0201, -6.4276, -6.2865, -6.2159,\n",
       "           -6.1993, -6.1657, -6.2964, -6.0646, -6.0045, -6.3327, -5.8757, -5.9727,\n",
       "           -6.1719, -5.8347, -6.2585, -6.3409, -6.2648, -6.3191, -6.1840, -5.9159,\n",
       "           -6.0311, -6.3416, -6.0329, -6.0774, -6.0743, -6.2745, -6.3355, -6.2741,\n",
       "           -6.0596, -5.9329, -6.2437, -6.2318, -5.9982, -6.1942, -6.1108, -6.1151,\n",
       "           -6.1543, -6.2090, -6.1173, -6.1764, -5.9905, -6.0618, -6.2233, -6.3346,\n",
       "           -5.9470, -6.2513, -5.9447, -6.2588, -6.1463, -6.1882, -6.0959, -5.9040,\n",
       "           -6.3475, -6.1871, -6.0278, -6.1953, -6.3407, -6.1473, -6.2086, -5.8523,\n",
       "           -5.9546, -6.1167, -5.8707, -6.2407, -6.0729, -6.3597, -6.2622, -6.3786,\n",
       "           -6.0089, -6.0759, -6.0587, -6.1400, -6.1713, -5.9773, -6.1092, -5.9320,\n",
       "           -6.2973, -5.7820, -5.9037, -6.1507, -6.0826, -6.2821, -6.1103, -5.9466,\n",
       "           -6.0500, -6.3290, -6.2752, -6.3622, -6.2119, -6.2763, -6.3325, -5.7989,\n",
       "           -6.2302, -6.1689, -5.9595, -6.2261, -5.8789, -6.1969, -6.0702, -5.9426,\n",
       "           -6.1721, -5.9259, -6.1112, -6.3497, -6.3537, -6.2182, -6.1448, -6.2884,\n",
       "           -6.1219, -5.8873, -6.1266, -6.3145, -5.6655, -6.2106, -5.6195, -6.1608,\n",
       "           -5.9457, -6.1013, -6.1575, -6.2129, -6.1573, -6.0250, -5.9258, -6.1162,\n",
       "           -6.2173, -6.3330, -6.0267, -6.1288, -6.4111, -5.8797, -6.2078, -5.9388,\n",
       "           -6.0994, -5.9991, -6.0299, -6.4437, -6.0578, -6.0548, -6.0860, -6.0353,\n",
       "           -6.2198, -5.9538, -6.3184, -6.2922, -5.9042, -6.0423, -6.3091, -5.9844,\n",
       "           -5.7595, -6.0309, -6.3649, -6.0736, -6.2578, -6.2742, -6.0870, -5.8822,\n",
       "           -5.9751, -6.3163, -5.9500, -6.1484, -6.1433, -5.9689, -6.0042, -6.1642,\n",
       "           -6.1190, -6.2137, -6.0025, -5.9777, -6.2821, -6.3826, -6.1071, -6.0319,\n",
       "           -6.2380, -6.4053, -6.3272, -5.9754, -6.1679, -6.2587, -6.1156, -6.1981,\n",
       "           -5.7539, -6.1774, -6.2450, -6.0683, -5.7925, -6.3163, -6.2529, -6.1317,\n",
       "           -6.2116, -6.1527, -6.2975, -6.2325, -6.0832, -6.1547, -5.0630, -6.2430,\n",
       "           -6.3274, -5.7867, -6.1826, -5.9034, -6.2315, -6.0439, -6.1211, -6.2279,\n",
       "           -6.3425, -6.2044, -6.2509, -5.9332, -6.4472, -6.3132, -6.2770, -5.6937,\n",
       "           -6.2683, -6.3266, -6.3600, -6.1375, -6.0403, -6.0914, -6.3854, -5.8361,\n",
       "           -6.0422, -6.0869, -6.0088, -6.3456, -6.3083, -6.2486, -6.1146, -6.2269,\n",
       "           -6.2000, -5.9964, -6.3150, -6.0336, -6.2688, -6.1690, -6.0504, -6.3115,\n",
       "           -6.0001, -6.1575, -6.0614, -6.1396, -6.2256, -6.2517, -5.8058, -6.1453,\n",
       "           -5.8210, -6.1450, -6.0682, -5.8853, -6.3258, -6.2786, -6.2089, -5.8467,\n",
       "           -6.0129, -6.0419, -6.2888, -6.1106, -6.2825, -6.2562, -6.2459, -5.9956,\n",
       "           -5.9527, -6.2115, -6.3061, -6.0918, -6.3090, -5.9677, -6.3268, -5.5718,\n",
       "           -6.0806, -6.3289, -5.9171, -6.0072, -6.0182, -6.2630, -6.2721, -6.3857,\n",
       "           -6.1543, -5.9031, -6.3993, -6.1887, -6.1359, -6.3063, -6.3299, -6.1141,\n",
       "           -6.3338, -5.9884, -6.3581, -6.0774, -6.4655, -6.2208, -6.2333, -6.3090,\n",
       "           -6.1407, -6.0037, -6.2651, -6.3151, -6.1109, -6.3951, -6.0253, -6.1974,\n",
       "           -6.0675, -6.1744, -6.4378, -6.0989, -6.3031, -6.4438, -6.2471, -6.2201,\n",
       "           -6.1345, -6.3586, -6.1993, -5.9426, -6.0615, -6.4143, -6.0905, -6.0555,\n",
       "           -6.3633, -6.3293, -5.9827, -6.2353, -5.8984, -6.2772, -6.1770, -5.7605,\n",
       "           -6.1127, -6.0710, -6.1064, -6.1174, -6.2652, -6.0229, -6.4166, -6.0560,\n",
       "           -6.1922, -6.1853, -5.9277, -6.0411, -6.4814, -6.4002, -5.8494, -6.0217,\n",
       "           -6.2339, -6.0989, -6.0683, -6.1276, -6.2586, -6.2112, -6.3387, -6.1860,\n",
       "           -6.1602, -6.3314, -6.3291, -6.2108, -6.1279, -6.3147, -5.8384, -6.0453,\n",
       "           -5.9289, -5.8503, -6.3527, -6.1684, -5.7705, -6.2593, -6.1656, -6.3980,\n",
       "           -5.7791, -6.2374, -5.7988, -6.0595, -6.1823, -6.3444, -6.3131, -6.2072,\n",
       "           -6.1902, -6.4742, -6.1171, -6.2095, -5.7297, -6.0303, -6.0428, -6.2410,\n",
       "           -5.9400, -5.7165, -6.1722, -5.8286, -6.2620, -5.6167, -6.2648, -6.3807,\n",
       "           -6.0248, -6.2929, -5.6610, -6.0550, -6.0844, -6.1167, -6.0703, -6.0546,\n",
       "           -5.8633, -5.8373, -5.9400, -6.2288, -6.1424, -5.6047, -6.2702, -5.9035,\n",
       "           -6.3077, -6.1369, -6.1044, -6.0697, -6.3861, -6.1519, -6.1318, -6.3526,\n",
       "           -5.7116, -5.9435, -5.9921, -6.0531, -6.2272, -6.2340, -5.9965, -6.1049,\n",
       "           -6.0864, -5.9211, -5.9158, -6.0973, -6.2732, -5.8800, -6.2061, -5.9582,\n",
       "           -6.1151, -6.3269, -6.2250, -6.1192, -6.1346, -5.9752, -6.1771, -5.9467,\n",
       "           -6.2505, -6.0321, -6.1720, -6.3516, -6.2770, -6.1978, -6.2636, -6.3485,\n",
       "           -5.8279, -6.1455, -6.1774, -6.2035, -6.1015, -6.2231, -6.3454, -6.0281,\n",
       "           -6.1790, -6.2329, -6.1795, -6.3801, -6.2493, -5.5845, -6.1977, -6.3485,\n",
       "           -5.8370, -6.2103, -6.2775, -5.6864, -6.2758, -6.4468, -6.1399, -6.0849,\n",
       "           -6.1800, -6.0787, -5.8653, -5.9103, -6.2533, -5.9568, -6.0846, -6.0661,\n",
       "           -6.0900, -6.3605, -5.8492, -6.0941, -6.3469, -6.3776, -6.0959, -6.1993,\n",
       "           -6.1520, -6.4215, -6.1325, -6.2971, -6.2649, -6.3266, -6.1848, -6.4236,\n",
       "           -6.1043, -6.3211, -6.3780, -6.3662, -6.0986, -6.3501, -6.0564, -6.0508,\n",
       "           -6.1649, -6.3361, -5.9852, -6.0487, -6.2380, -6.1378, -5.8208, -6.2096,\n",
       "           -6.2373, -5.9878, -5.5837, -6.2884, -5.8719, -5.9870, -6.0667, -6.3890,\n",
       "           -6.1544, -5.9413, -6.2666, -5.8194, -6.0289, -6.1824, -6.3273, -6.1559,\n",
       "           -6.1928, -5.9961, -5.6220, -6.1775, -6.1453, -6.1341, -6.0731, -6.4203,\n",
       "           -5.9893, -6.2928, -6.2662, -5.9227, -6.3166, -6.0539, -5.9082, -6.0672,\n",
       "           -5.8232, -5.7659, -6.1556, -6.0887, -6.1503, -5.8824, -6.0793, -6.1179,\n",
       "           -6.1394, -6.2699, -6.0914, -6.3453, -6.2914, -6.1535, -5.8776, -5.9450,\n",
       "           -6.2905, -5.7856, -6.2742, -6.1294, -6.0174, -6.1564, -6.1959, -6.1504]),\n",
       "   'acceptance_rate': tensor([]),\n",
       "   'penalty': None}],\n",
       " 'args': {'positional arguments': Namespace(),\n",
       "  'optional arguments': Namespace(help=None),\n",
       "  'Main args': Namespace(energy_module='MinGapEnergy', energy_threshold=-0.5, generator_module='SimulatedAnnealing', max_attempts=40, monitor=None, n_proposals=[1000], params_module='BasicParameters', penalty_module=None, proposal_path='/tmp/test__k562__sa', reset_params=True, tolerate_unknown_args=False),\n",
       "  'Params Module args': Namespace(batch_dim=0, batch_size=256, cat_axis=-1, init_seqs=None, left_flank='GTACGGGAGGTATTGGACAGGCCGCAATAAAATATCTTTATTTTCATTACATCTGTGTGTTGGTTTTTTGTGTGAATCGATAGTACTAACATACGCTCTCCATCAAAACAAAACGAAACAAAACAAACTAGCAAAATAGGCTGTCCCCAGTGCAAGTGCAGGTGCCAGAACATTTCTCTGGCCTAACTGGCCGCTTGACG', length=200, n_channels=4, right_flank='CACTGCGGCTCCTGCGATCTAACTGGCCGGTACCTGAGCTCGCTAGCCTCGAGGATATCAAGATCTGGCCTCGGCGGCCAAGCTTAGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTTGGTAAAGCCACCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATC', token_dim=-2),\n",
       "  'Energy Module args': Namespace(a_max=6.0, a_min=-2.0, bending_factor=0.0, model_artifact='/tmp/model_artifacts__20240216_170930__404251.tar.gz', target_alpha=1.0, target_feature=0),\n",
       "  'Generator Constructor args': Namespace(a=1.0, b=1.0, gamma=0.501, n_positions=5),\n",
       "  'Generator Runtime args': Namespace(keep_burnin=False, n_burnin=0, n_steps=2000)},\n",
       " 'timestamp': '20240216_171729',\n",
       " 'random_tag': 147400}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sa_props = !ls /tmp/\n",
    "sa_props = [ p for p in sa_props if 'test__k562__sa' in p ][-1]\n",
    "torch.load(f'/tmp/{sa_props}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
